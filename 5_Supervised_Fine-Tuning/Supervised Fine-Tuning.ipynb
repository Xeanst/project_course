{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным этапом обучения больших языковых моделей является их тонкая настройка для решения широкого круга задач; этот метод известен как тонкая настройка с учителем (supervised fine-tuning, SFT). Данный процесс помогает моделям стать более универсальными и способными работать при различных вариантах использования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install transformers datasets trl huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from IPython.core.display import display, HTML\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aвторизация на Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаблоны чатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблоны чатов (chat templates) необходимы для структурирования взаимодействия между языковыми моделями и пользователями. Понимание того, как правильно форматировать разговоры, имеет решающее значение для получения наилучших результатов модели. Рассмотрим, что такое шаблоны чата, почему они важны и как их использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовая модель vs. инструктивная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая модель обучается на необработанных текстовых данных, чтобы предсказать следующий токен. Инструктивная модель настраивается специально для выполнения инструкций и участия в разговорах. Например, [SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) является базовой моделью, в то время как [SmolLM2-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) – это ее вариант с дообучением на инструкциях.\n",
    "\n",
    "Модели, настроенные на инструкции, обучаются следовать определенной структуре диалога, что делает их более подходящими для построения виртуальных ассистентов. Кроме того, инструктивные модели могут обрабатывать сложные взаимодействия, включая использование внешних инструментов, мультимодальный ввод данных и вызов функций.\n",
    "\n",
    "Чтобы базовая модель стала инструктивной, нам нужно отформатировать промпты определенным образом, понятным модели. Вот тут-то и пригодятся шаблоны чата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распространенные форматы шаблонов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем понять, как различные модели ожидают, что их диалоги будут отформатированы. Рассмотрим некоторые распространенные форматы шаблонов на простом примере диалога.\n",
    "\n",
    "Для всех примеров будем использовать следующую структуру диалога:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это шаблон ChatML, используемый в таких моделях, как SmolLM2 и Qwen 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "after_formating = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "print(after_formating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так используется шаблон Mistral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "after_formating = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "print(after_formating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные различия между этими форматами следующие:\n",
    "\n",
    "1. Обработка системных сообщений\n",
    "- Llama 2 оборачивает системные сообщения в теги `<<SYS>>`\n",
    "- В Llama 3 используются теги `<|system|>` и `</s>`\n",
    "- Mistral включает системное сообщение в первый промпт\n",
    "- Qwen использует явную системную роль с тегами `<|im_start|>`\n",
    "- ChatGPT использует префикс `SYSTEM:`\n",
    "\n",
    "2. Границы сообщений\n",
    "- В Llama 2 используются теги `[INST]` и `[/INST]`\n",
    "- В Llama 3 используются теги, относящиеся к конкретной роли (`<|system|>`, `<|user|>`, `<|assistant|>`), с окончаниями `</s>`\n",
    "- Mistral использует теги `[INST]` и `[/INST]` с `<s>` и `</s>`\n",
    "\n",
    "3. Специальные токены\n",
    "- Llama 2 использует `<s>` и `</s>` для обозначения границ разговора\n",
    "- Llama 3 использует `</s>` для завершения каждого сообщения\n",
    "- Mistral использует `<s>` и `</s>` для обозначения границ\n",
    "- Qwen использует начальные и конечные токены, зависящие от конкретной роли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расширенные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблоны чата могут работать с более сложными сценариями, выходящими за рамки простого общения, включая:\n",
    "- Использование внешних инструментов или API\n",
    "- Мультимодальный ввод для обработки изображений, аудио\n",
    "- Вызов функции для выполнения структурированного алгоритма\n",
    "- Контекст из нескольких реплик для ведения истории диалога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для мультимодальных диалогов шаблоны чата могут включать ссылки на изображения или изображения в кодировке base64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful vision assistant that can analyze images.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\"type\": \"image\", \"image_url\": \"https://example.com/image.jpg\"},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример шаблона чата с использованием внешнего инструмента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me help you with that.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"tool\": \"calculator\",\n",
    "                \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456},\n",
    "            },\n",
    "            {\"tool\": \"weather_api\", \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}},\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"},\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_name\": \"weather_api\",\n",
    "        \"content\": \"{'condition': 'rain', 'temperature': 15}\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие принципы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с шаблонами чата требуется следовать нескольким основным рекомендациям.\n",
    "1. Единообразное форматирование: на протяжении всего проекта дожен использоваться один и тот же шаблон чата\n",
    "2. Четкое определение ролей: роли (`system`, `user`, `assistant`, `tool`) должны быть указаны для каждого сообщения\n",
    "3. Управление контекстом: необходимо помнить об ограничениях на количество токенов при ведении истории разговоров\n",
    "4. Обработка ошибок: требуется тщательно продумать обработку ошибок для вызовов внешних инструментов и для мультимодальных входных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое упражнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попрактикуемся в реализации шаблонов чата на реальном примере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаблон чата SmolLM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим модель и токенизатор\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим сообщение для SmolLM2\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение шаблона чата без токенизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизатор представляет диалог в виде строки со специальными токенами, описывающими роль пользователя и ассистента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "print(\"Conversation with template:\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Декодирование диалога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что диалог представлен так же, как и выше, но с дополнительным сообщением ассистента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация диалога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизатор сопоставляет токенам диалога и специальным токенам идентификаторы из словаря модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "print(\"Conversation tokenized:\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование данных для SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем набор данных [smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) из Hugging Face и предобработаем его для тонкой настройки с учителем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        \"\"\"<iframe\n",
    "  src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"360px\"\n",
    "></iframe>\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем каждое сообщение в формат чата с помощью токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(sample):\n",
    "    sample[\"no_tokenization\"] = tokenizer.apply_chat_template(sample[\"messages\"], tokenize=False, add_generation_prompt=True)\n",
    "    sample['tokenized'] = tokenizer.apply_chat_template(sample[\"messages\"], add_generation_prompt=True)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(process_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ds[\"train\"][0].items():\n",
    "  print(f\"{k}:\\n{v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы узнали, как применять шаблоны чата к различным моделям. Структурируя взаимодействия с помощью шаблонов чата, мы можем гарантировать, что языковые модели будут выдавать согласованные и контекстуально значимые ответы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тонкая настройка с учителем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный процесс используется для адаптации предобученных базовых языковых моделей для выполнения инструкций, ведения диалога и использования определенных выходных форматов. Несмотря на то, что предварительно обученные модели обладают впечатляющими общими возможностями, SFT помогает преобразовать их в модели, похожие на виртуальных ассистентов, которые могут лучше понимать запросы пользователя и реагировать на них. Обычно это достигается путем обучения на основе наборов данных с написанными человеком разговорами и инструкциями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Когда и для чего использовать SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFT позволяет точно контролировать структуру выходных данных модели. Это особенно ценно, когда нужно, чтобы модель:\n",
    "1. Генерировала ответы в определенном формате шаблона чата\n",
    "2. Строго следовала схемам вывода\n",
    "3. Поддерживала единообразный стиль ответов\n",
    "\n",
    "SFT помогает привести модель в соответствие с требованиями конкретной предметной области посредством:\n",
    "\n",
    "1. Обучения терминологии и концепциям предметной области\n",
    "2. Предписания профессиональных стандартов\n",
    "3. Надлежащей обработке технических запросов\n",
    "4. Следования отраслевым рекомендациям\n",
    "\n",
    "Для тонкой настройки с учителем требуется набор данных из пар ввода-вывода. Каждая пара должна состоять из:\n",
    "1. Входного промпта\n",
    "2. Ожидаемого ответа модели\n",
    "3. Любой дополнительной информации или метаданных\n",
    "\n",
    "Качество обучающих данных имеет решающее значение для успешной тонкой настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурация обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Успех тонкой настройки также во многом зависит от правильного выбора гиперпараметров обучения.\n",
    "1. Параметры, определяющие продолжительность обучения\n",
    "- `num_train_epochs`: контролирует общую продолжительность обучения\n",
    "- `max_steps`: аьтернатива эпохам, устанавливает максимальное количество этапов обучения\n",
    "- большее количество эпох позволяет лучше обучаться, но сопряжено с риском переобучения\n",
    "\n",
    "2. Параметры, определяющие размер батча\n",
    "- `per_device_train_batch_size`: от него зависит использование памяти и стабильность обучения\n",
    "- `gradient_accumulation_steps`: позволяет обновлять веса не после каждого батча, а после заданного количества этапов\n",
    "- больший размер батча обеспечивает более стабильное обучение, но требует больше памяти\n",
    "\n",
    "3. Параметры, определяющие скорость обучения\n",
    "- `learning_rate`: контролирует силу обновлений весов\n",
    "- `warmup_ratio`: определяет долю обучающей выборки, которая используется для увеличения скорости обучения\n",
    "- слишком высокое значение может привести к нестабильности, слишком низкое — к медленному обучению\n",
    "\n",
    "4. Параметры, которые позволяют отслеживать процесс обучения\n",
    "- `logging_steps`: частота вывода метрики\n",
    "- `eval_steps`: как часто проводить оценку на основе валидационных данных\n",
    "- `save_steps`: частота сохранения контрольной точки модели\n",
    "\n",
    "Стоит начать с базовых значений и корректировать их на основе мониторинга:\n",
    "- 1-3 эпохи\n",
    "- небольшой размер батча\n",
    "\n",
    "Следует внимательно следить за показателями на валидационной выборке и корректировать скорость обучения, если оно нестабильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отслеживание процесса обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение **функции потерь (loss function)** обычно изменяется в три этапа:\n",
    "\n",
    "- Начальный резкий спад: быстрая адаптация к новому распределению данных\n",
    "- Постепенная стабилизация: скорость обучения замедляется по мере доработки модели\n",
    "- Сходимость: значение функции потерь стабилизируется, что указывает на завершение обучения\n",
    "\n",
    "На приведенном графике показан типичный процесс обучения. Обратите внимание, что значение функции потерь на обучении и на валидации снижается сначала резко, а затем постепенно. Эта закономерность указывает на то, что модель эффективно обучается, сохраняя способность к обобщению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/vBxgYCDv/loss.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эффективный мониторинг предполагает отслеживание количественных показателей и оценку качественных показателей, таких как:\n",
    "\n",
    "- значение функции потерь на обучающей выборке (training loss)\n",
    "- значение функции потерь на валидационной выборке (validation loss)\n",
    "- изменение скорости обучения (learning rate progression)\n",
    "\n",
    "По мере прохождения обучения график функции потерь должен постепенно стабилизироваться. Ключевым показателем правильного обучения является небольшой разрыв между значением функции потерь при обучении и при валидации. Это позволяет предположить, что модель изучает обобщаемые закономерности, а не запоминает конкретные примеры. Абсолютные значения функции потерь будут варьироваться в зависимости от вашей задачи и набора данных.\n",
    "\n",
    "Необходимо следить за несколькими предупреждающими знаками во время обучения:\n",
    "1. Значение функции потерь при валидации увеличивается, а при обучении уменьшается (переобучение)\n",
    "2. Значение функции потерь незначительно увеличивается (недостаточное обучение)\n",
    "3. Чрезвычайно низкое значение функции потерь (потенциальное запоминание)\n",
    "4. Несогласованное форматирование выходных данных (проблемы с усвоением шаблонов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если значение функции потерь при валидации уменьшается значительно медленнее, чем при обучении, то  модель, скорее всего, подстраивается под данные обучения. Чтобы это исправить, можно:\n",
    "- Сократить количество этапов обучения\n",
    "- Увеличить размер набора данных\n",
    "- Проверить качество и разнообразие набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/CxWVytT0/loss1.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если значения функции потерь не показывают существенного улучшения, возможно, модель:\n",
    "\n",
    "- Обучается слишком медленно (попробуйте увеличить скорость обучения)\n",
    "- Возникают трудности с задачей (проверьте качество данных и сложность задачи)\n",
    "- Возникают архитектурные ограничения (рассмотрите другую модель)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/RZJjTR3D/loss2.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чрезвычайно низкое значение функции потерь может свидетельствовать скорее о запоминании, чем об обучении. Это особенно важно, если:\n",
    "\n",
    "- Модель плохо работает на новых, похожих примерах\n",
    "- В выходных данных отсутствует разнообразие\n",
    "- Ответы слишком похожи на обучающие примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/ZqFsfDJV/loss3.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое упражнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как осуществить тонкую настройку модели [SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) с помощью [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) из библиотеки [TRL](https://huggingface.co/docs/trl/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/KjrnbCsH/trl-banner-dark.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим модель и токенизатор\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация с помощью базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим промпт\n",
    "prompt = \"Hello! How are you?\"\n",
    "\n",
    "# Отформатируем его\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Сгенерируем ответ\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим набор данных и отформатируем его для обучения. Набор данных должен содержать пары ввода-вывода, где каждый ввод является промптом, а вывод — ожидаемым ответом модели.\n",
    "\n",
    "Библиотека TRL форматирует входные сообщения на основе шаблонов чата модели. Они должны быть представлены в виде списка словарей с ключами `role` и `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение параметров для SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В SFTTrainer настраиваются различные параметры, которые управляют процессом обучения. К ним относится количество эпох обучения, размер батча, скорость обучения и способ оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка параметров\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # корректирется в зависимости от размера набора данных и желаемой продолжительности обучения\n",
    "    per_device_train_batch_size=4,  # устанавливается в соответствии с объемом памяти графического процессора\n",
    "    learning_rate=5e-5,  # отправная точка для тонкой настройки\n",
    "    logging_steps=10,  # частота отслеживания показателей обучения\n",
    "    save_steps=100,  # частота сохранения модели\n",
    "    logging_strategy=\"steps\",  # оценивает модель через регулярные промежутки времени\n",
    "    eval_steps=50,  # частота валидации\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Инициализация\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    processing_class=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация с помощью дообученой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим тот же промт\n",
    "prompt = \"Hello! How are you?\"\n",
    "\n",
    "# Отформатируем его\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Сгенерируем ответ\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели пошаговое руководство по тонкой настройке модели SmolLM2-135M с помощью SFTTrainer. Эти этапы позволили эффективно адаптировать модель для выполнения конкретных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После тонкой настройки модели с помощью SFT мы должны оценить ее с помощью ряда стандартных бенчмарков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоматическая оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоматические бенчмарки служат стандартизированными инструментами для оценки языковых моделей с учетом различных задач и возможностей. Хотя они являются полезной отправной точкой для понимания эффективности моделей, важно понимать, что они представляют собой лишь часть комплексной стратегии оценки.\n",
    "\n",
    "Автоматические бенчмарки обычно состоят из тщательно отобранных наборов данных с определенными задачами и оценочными метриками. Они направлены на оценку различных аспектов возможностей модели, от базового понимания языка до сложных рассуждений. Ключевым преимуществом использования автоматических бенчмарков является их стандартизация — они позволяют проводить последовательное сравнение различных моделей и обеспечивают воспроизводимость результатов.\n",
    "\n",
    "Однако важно понимать, что результаты не всегда напрямую влияют на эффективность в реальном мире. Модель, которая отлично справляется с академическими бенчмарками, может по-прежнему испытывать трудности с приложениями в конкретной предметной области или практическим использованием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка общих знаний о мире"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MMLU](https://huggingface.co/datasets/cais/mmlu) (Massive Multitask Language Understanding) проверяет знания по 57 предметам, от естественных до гуманитарных. Хотя бенчмарк и является разносторонним, он может не отражать глубину знаний, необходимых для конкретных областей. Генерация ответа на вопрос оценивает склонность модели воспроизводить распространенные заблуждения, хотя и не может охватить все формы дезинформации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu = load_dataset(path=\"cais/mmlu\", name=\"machine_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка способности к рассуждению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BBH](https://huggingface.co/datasets/lukaemon/bbh) (Big Bench Hard) и [GSM8K](https://huggingface.co/datasets/openai/gsm8k) (Grade School Math 8K) ориентированы на сложные логические задачи. BBH тестирует логическое мышление и планирование, в то время как GSM8K ориентирован на решение математических задач. Эти бенчмарки помогают оценить аналитические способности, но могут не отражать нюансы мышления, необходимые в реальных сценариях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = load_dataset(path=\"lukaemon/bbh\", name=\"temporal_sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(path=\"openai/gsm8k\", name=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бенчмарки, относящиеся к конкретной предметной области"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бенчмарк [MATH](https://huggingface.co/papers/2103.03874) — это еще один важный инструмент оценки математического мышления. Он состоит из 12 500 задач из соревнований по математике, охватывающих алгебру, геометрию, теорию чисел, теорию вероятностей и многое другое. Что делает бенчмарк особенно сложным, так это то, что он требует многоэтапного рассуждения, понимания формальных математических обозначений и умения генерировать пошаговые решения. В отличие от простых арифметических задач, математические задачи часто требуют сложных стратегий решения задач и применения математических концепций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math = load_dataset(path=\"qwedsacf/competition_math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math[\"train\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HumanEval Benchmark](https://huggingface.co/datasets/openai/openai_humaneval) — это набор данных, состоящий из 164 задач по программированию. Он проверяет способность модели генерировать функционально корректный код на Python, который решает поставленные задачи программирования. Что делает HumanEval особенно ценным, так это то, что он оценивает как возможности генерации кода, так и функциональную корректность на основе реального выполнения тестовых примеров, а не просто поверхностного сходства с эталонными решениями. Задачи варьируются от базовых манипуляций со строками до более сложных алгоритмов и структур данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval = load_dataset(path=\"openai/openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in human_eval[\"test\"][0].items():\n",
    "  print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) — это автоматизированная система оценки, разработанная для оценки инструктивных языковых моделей. Она использует GPT-4 в качестве критерия для оценки результатов генерации по различным параметрам, включая полезность (helpfulness), честность (honesty) и безвредность (harmlessness). Фреймворк включает в себя набор данных из 805 тщательно отобранных запросов. Финальная метрика показывает, как часто результаты оцениваемой модели оказались предпочтительнее результатов эталонной модели. Что делает Alpaca Eval особенно полезным, так это его способность предоставлять согласованные, масштабируемые оценки без использования аннотаторов-людей, при этом сохраняя нюансы производительности модели, которые могут быть упущены при использовании традиционных показателей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Альтернативные подходы к оценке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были разработаны альтернативные методы оценки, чтобы устранить ограничения стандартных бенчмарков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель-арбитр (LLM-as-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование одной языковой модели для оценки результатов другой становится все более популярным. Этот подход может обеспечить более детальную обратную связь, чем традиционные показатели, хотя и имеет свои недостатки и ограничения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Арены для оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Арены для оценки как [Chatbot Arena](https://lmarena.ai/) предлагают уникальный подход к оценке LLM с помощью краудсорсинговой обратной связи. На этих платформах пользователи участвуют в анонимных “битвах” между двумя LLM, задавая вопросы и голосуя за то, какая модель дает лучшие ответы. Этот подход отражает реальные паттерны использования и предпочтения пользователей с помощью разнообразных, сложных вопросов, а исследования показывают сильное соответствие между голосованиями из краудсорсинга и экспертными оценками. Несмотря на свою мощь, эти платформы имеют свои ограничения, в том числе потенциальную предвзятость в отношении базы пользователей, неравномерное распределение запросов и основное внимание на полезности, а не на безопасность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пользовательские бенчмарки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Организации часто разрабатывают внутренние наборы тестов, адаптированные к их конкретным потребностям и вариантам использования. Они могут включать вопросы на знание предметной области или сценарии оценки, которые отражают реальные условия использования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы изучили основные компоненты тонкой настройки языковых моделей:\n",
    "1. **Шаблоны чатов** предоставляют структуру для моделирования взаимодействий, обеспечивая согласованность и адекватность ответов благодаря стандартизированному форматированию.\n",
    "2. **Тонкая настройка с учителем (SFT)** позволяет адаптировать предварительно обученные модели к конкретным задачам, сохраняя при этом их базовые знания.\n",
    "3. **Оценка** помогает измерить и подтвердить эффективность тонкой настройки с помощью различных показателей и бенчмарков."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
