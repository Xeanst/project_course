{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вход на Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "username = HfApi().whoami()[\"name\"]\n",
    "REPO_NAME = f\"{username}/RuSENTNE_project\"\n",
    "\n",
    "print(f\"Homework repository: '{REPO_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/train_data.csv\n",
    "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/validation_data_labeled.csv\n",
    "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/final_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train_data.csv', sep='\\t')\n",
    "validation = pd.read_csv('validation_data_labeled.csv', sep='\\t')\n",
    "test = pd.read_csv('final_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перефразирование обучающей выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модель для перефразирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model.cuda();\n",
    "model.eval();\n",
    "\n",
    "def paraphrase(text, beams=5, grams=4, do_sample=False):\n",
    "    x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\n",
    "    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
    "    out = model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(paraphrase('Каждый охотник желает знать, где сидит фазан.'))\n",
    "# Все охотники хотят знать где фазан сидит.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как работает модель, на примере из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paraphrase('Владислав первым заметил возгорание и начал тушить его.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перефразируем предложения положительного и отрицательного класса из выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(1863))\n",
    "\n",
    "new_sentences = []\n",
    "for i, sentence in enumerate(train[\"sentence\"]):\n",
    "  if train[\"label\"][i] != 0:\n",
    "    new_sent = paraphrase(sentence)\n",
    "    progress_bar.update(1)\n",
    "    if new_sent != sentence:\n",
    "      new_row = {\"sentence\": new_sent,\n",
    "                 \"entity\": train[\"entity\"][i],\n",
    "                  \"entity_tag\": train[\"entity_tag\"][i],\n",
    "                  \"entity_pos_start_rel\": train[\"entity_pos_start_rel\"][i],\n",
    "                  \"entity_pos_end_rel\": train[\"entity_pos_end_rel\"][i],\n",
    "                  \"label\": train[\"label\"][i]}\n",
    "      new_sentences.append(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датафрейм с новыми предложениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = pd.DataFrame(new_sentences, columns=['sentence', 'entity', 'entity_tag', 'entity_pos_start_rel', 'entity_pos_end_rel', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним полученные предложения в csv-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension.to_csv('extension.csv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда после перефразирования исходная сущность отсутствует в предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extension[\"sentence\"][100])\n",
    "print(extension[\"entity\"][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расширение обучающей выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим изначальную обучающую выборку с перефразированными предложениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = pd.read_csv('extension.csv', sep='\\t')\n",
    "train_extended = pd.concat([train, extension], ignore_index=True)\n",
    "train_extended.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемешаем полученную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extended = train_extended.sample(frac=1, random_state=22).reset_index(drop=True)\n",
    "train_extended.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def question(df, sent='Как относятся к {}?', c='datv'):\n",
    "  sentences = []\n",
    "  for entity in df['entity'].values:\n",
    "    try:\n",
    "      dative_list = [pymorphy3.shapes.restore_capitalization(morph.parse(x)[0].inflect({c}).word, x) for x in entity.split()]\n",
    "      final_form = ' '.join(dative_list)\n",
    "    except AttributeError:\n",
    "      final_form = entity\n",
    "    sentences.append(sent.format(final_form))\n",
    "  return sentences\n",
    "\n",
    "train_extended['question'] = question(train_extended, 'Как относятся к {}?')\n",
    "validation['question'] = question(validation, 'Как относятся к {}?')\n",
    "test['question'] = question(test, 'Как относятся к {}?')\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {-1: 0, 0: 1, 1: 2}\n",
    "train_extended['raw_label'] = train_extended[\"label\"]\n",
    "train_extended['label'] = train_extended[\"raw_label\"].map(label_dict)\n",
    "validation['raw_label'] = validation[\"label\"]\n",
    "validation['label'] = validation[\"raw_label\"].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": Dataset.from_pandas(train_extended),\n",
    "                            \"validation\": Dataset.from_pandas(validation),\n",
    "                            \"test\": Dataset.from_pandas(test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение обычной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(output_dir=f'{REPO_NAME}-base', push_to_hub=True, evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(f\"{REPO_NAME}-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем предсказания модели на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_labels(dataset):\n",
    "    output = trainer.predict(dataset)\n",
    "    logits, labels = output[:2]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
    "    return [reverse_label_dict[x] for x in predictions]\n",
    "\n",
    "validation_predictions = predict_labels(tokenized_dataset[\"validation\"])\n",
    "print(len(validation_predictions))\n",
    "validation_predictions[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество модели на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
    "\n",
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
    "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
    "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
    "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
    "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сохраним предсказания модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_labels(tokenized_dataset[\"test\"])\n",
    "print(len(test_predictions))\n",
    "test_predictions[1925:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_base.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_base.csv'}, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовой выборке на платформе CodaLab для базовой модели было получено качество 54.21 по макро F1-мере для положительного и отрицательного классов, 64.77 - для трех классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение предобученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем дообучить разные модели (предобученные на анализ тональности предложений) для анализа тональности именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\", num_labels=3).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seara/rubert-base-cased-russian-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seara/rubert-base-cased-russian-sentiment\", num_labels=3).to(device)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"r1char9/rubert-base-cased-russian-sentiment\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"r1char9/rubert-base-cased-russian-sentiment\", num_labels=3).to(device)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny-sentiment-balanced\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny-sentiment-balanced\", num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(output_dir=f\"{REPO_NAME}-cointegrated\",push_to_hub=True, evaluation_strategy=\"epoch\", num_train_epochs=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(f\"{REPO_NAME}-seara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказания на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_labels(dataset):\n",
    "    output = trainer.predict(dataset)\n",
    "    logits, labels = output[:2]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
    "    return [reverse_label_dict[x] for x in predictions]\n",
    "\n",
    "validation_predictions = predict_labels(tokenized_dataset[\"validation\"])\n",
    "print(len(validation_predictions))\n",
    "print('{0}'.format(validation_predictions[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчёт качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
    "\n",
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
    "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
    "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
    "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
    "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты для **r1char9/rubert-base-cased-russian-sentiment** на валидационной выборке:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Макро F1-мера:                                                     0.66.\n",
    "Макро F1-мера для положительного и отрицательного классов:         0.42.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты для **seara/rubert-base-cased-russian-sentiment** на валидационной выборке:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Макро F1-мера:                                                     0.67.\n",
    "Макро F1-мера для положительного и отрицательного классов:         0.43.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты для **blanchefort/rubert-base-cased-sentiment** на валидационной выборке:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Макро F1-мера:                                                     0.28.\n",
    "Макро F1-мера для положительного и отрицательного классов:         0.0.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты для **cointegrated/rubert-tiny-sentiment-balanced** на валидационной выборке:\n",
    "\n",
    "```\n",
    "Макро F1-мера:                                                     0.55.\n",
    "Макро F1-мера для положительного и отрицательного классов:         0.39.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем предсказания моделей на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_labels(tokenized_dataset[\"test\"])\n",
    "print(len(test_predictions))\n",
    "test_predictions[1925:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_2.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
    "# pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_3.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
    "#pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_4.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
    "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_seara.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Промежуточные результаты моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
    "|----------|----------|----------|\n",
    "| Модель без предобучения  | 54.21   | 64.77   |\n",
    "| seara/rubert-base-cased-russian-sentiment    |  52.37  |  63.13  |\n",
    "| r1char9/rubert-base-cased-russian-sentiment    | 8.97   | 32.82   |\n",
    "| blanchefort/rubert-base-cased-sentiment    | 14.05   | 36.41   |\n",
    "| cointegrated/rubert-tiny-sentiment-balanced    | 30.91   | 46.81   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобученные модели на выборке без расширения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymorphy3\n",
    "!pip install -q datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def question(df):\n",
    "  sentences = []\n",
    "  for entity in df['entity'].values:\n",
    "    try:\n",
    "      dative_list = [pymorphy3.shapes.restore_capitalization(morph.parse(x)[0].inflect({'datv'}).word, x) for x in entity.split()]\n",
    "      final_form = ' '.join(dative_list)\n",
    "    except AttributeError:\n",
    "      final_form = entity\n",
    "    sentences.append(f'Как относятся к {final_form}?')\n",
    "  return sentences\n",
    "\n",
    "train['question'] = question(train)\n",
    "validation['question'] = question(validation)\n",
    "test['question'] = question(test)\n",
    "\n",
    "label_dict = {-1: 0, 0: 1, 1: 2}\n",
    "train['raw_label'] = train[\"label\"]\n",
    "train['label'] = train[\"raw_label\"].map(label_dict)\n",
    "validation['raw_label'] = validation[\"label\"]\n",
    "validation['label'] = validation[\"raw_label\"].map(label_dict)\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\": Dataset.from_pandas(train),\n",
    "                            \"validation\": Dataset.from_pandas(validation),\n",
    "                            \"test\": Dataset.from_pandas(test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "class MyTrainer():\n",
    "  def __init__(self, model, tokenizer, tokenized_dataset, training_arguments):\n",
    "    self.trainer = Trainer(\n",
    "        model,\n",
    "        training_arguments,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        processing_class=tokenizer\n",
    "    )\n",
    "\n",
    "  def training(self):\n",
    "    self.trainer.train()\n",
    "\n",
    "  def predict_labels(self, dataset):\n",
    "      output = self.trainer.predict(dataset)\n",
    "      logits, labels = output[:2]\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
    "      return [reverse_label_dict[x] for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "results = defaultdict(lambda: defaultdict(float))\n",
    "model_names = [\"blanchefort/rubert-base-cased-sentiment\", \"seara/rubert-base-cased-russian-sentiment\", \"r1char9/rubert-base-cased-russian-sentiment\", \"cointegrated/rubert-tiny-sentiment-balanced\"]\n",
    "\n",
    "def tokenize_function(example):\n",
    "  return tokenizer(example[\"question\"], example[\"sentence\"])\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "      metric = evaluate.load(\"f1\")\n",
    "      return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
    "\n",
    "def apply_to_model(model_name, training_arguments):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "  tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "  trainer = MyTrainer(model, tokenizer, tokenized_dataset, training_arguments)\n",
    "  trainer.training()\n",
    "  validation_predictions = trainer.predict_labels(tokenized_dataset[\"validation\"])\n",
    "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
    "  f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
    "  filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
    "  f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
    "  results[model_name][\"f1_PN0\"] = round(f1_score_all['f1'], 2)\n",
    "  results[model_name][\"f1_PN\"] = round(f1_score_filtered['f1'], 2)\n",
    "  test_predictions = trainer.predict_labels(tokenized_dataset[\"test\"])\n",
    "  pd.Series(test_predictions).to_csv('RuSentNE_predictions_{}.zip'.format(model_name[:4]), compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
    "\n",
    "for model_name in model_names:\n",
    "  # apply_to_model(model_name, TrainingArguments(output_dir='./results', eval_strategy=\"epoch\", num_train_epochs=3, weight_decay=0.01, learning_rate=1e-6))\n",
    "  apply_to_model(model_name, TrainingArguments(output_dir='./results', eval_strategy=\"epoch\", num_train_epochs=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Качество моделей при дефолтных значениях параметров weight_decay (0) и learning_rate (5e-5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_for_default_training_args = {\"blanchefort/rubert-base-cased-sentiment\": {\"f1_PN0\": 0.36, \"f1_PN\": 0.11},\n",
    "                                     \"seara/rubert-base-cased-russian-sentiment\": {\"f1_PN0\": 0.69, \"f1_PN\": 0.46},\n",
    "                                     \"r1char9/rubert-base-cased-russian-sentiment\t\": {\"f1_PN0\": 0.67, \"f1_PN\": 0.45},\n",
    "                                     \"cointegrated/rubert-tiny-sentiment-balanced\": {\"f1_PN0\": 0.54, \"f1_PN\": 0.33},\n",
    "}\n",
    "df_base_default = pd.DataFrame(results_for_default_training_args)\n",
    "df_base_default = df_base_default.transpose()\n",
    "f1_PN0_base = [0.28, 0.67, 0.66, 0.55]\n",
    "df_base_default['enlarged dataset f1_PN0'] = f1_PN0_base\n",
    "f1_PN_base = [0.0, 0.43, 0.42, 0.39]\n",
    "df_base_default['enlarged dataset f1_PN'] = f1_PN_base\n",
    "df_base_default = df_base_default.iloc[:, [0, 2, 1, 3]]\n",
    "\n",
    "df_base_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на CodaLab\n",
    "\n",
    "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
    "|----------|----------|----------|\n",
    "| seara/rubert-base-cased-russian-sentiment    |  52.37  |  63.13  |\n",
    "| cointegrated/rubert-tiny-sentiment-balanced    | 30.91   | 46.81   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамбль моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Trainer(model):\n",
    "  trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-base\")\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-base\")\n",
    "trainer1 = create_Trainer(model1)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-seara\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-seara\")\n",
    "trainer2 = create_Trainer(model2)\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-cointegrated\")\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-cointegrated\")\n",
    "trainer3 = create_Trainer(model3)\n",
    "models = [trainer2, trainer1, trainer3]\n",
    "tokenizers = [tokenizer2, tokenizer1, tokenizer3]\n",
    "\n",
    "def tokenize_function(example, tokenizer):\n",
    "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
    "\n",
    "def majority_voting(dataset, models, tokenizers):\n",
    "    predictions = []\n",
    "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
    "    for i, model in enumerate(models):\n",
    "        tokenizer = tokenizers[i]\n",
    "        tokenized_dataset = [tokenize_function(i, tokenizer) for i in dataset]\n",
    "        output = model.predict(tokenized_dataset)\n",
    "        logits, labels = output[:2]\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.array(predictions)\n",
    "    final_predictions = [np.bincount(pred).argmax() for pred in predictions.T]\n",
    "    return [reverse_label_dict[x] for x in final_predictions]\n",
    "validation_predictions = majority_voting(dataset_dict['validation'], models, tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].remove_columns([\"predictions\"])\n",
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
    "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
    "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
    "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
    "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = majority_voting(dataset_dict['test'], models, tokenizers)\n",
    "print(len(test_predictions))\n",
    "test_predictions[1925:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_ensemble.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_ensemble.csv'}, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на CodaLab\n",
    "\n",
    "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
    "|----------|----------|----------|\n",
    "| Ансамбль из трёх моделей  | 50.73     | 62.34   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Более сложный ансамбль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_model_predictions(models, tokenizer, dataset):\n",
    "\n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        tokenized_dataset = dataset.map(lambda example: tokenize_function(example, tokenizer[i]), batched=True)\n",
    "\n",
    "        output = model.predict(tokenized_dataset)\n",
    "        logits, labels = output[:2]\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        predictions.append(preds)\n",
    "\n",
    "    return np.stack(predictions, axis=1)\n",
    "\n",
    "def regression_ensemble(models, tokenizers, dataset, labels):\n",
    "    preds = get_model_predictions(models, tokenizers, dataset)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(preds, labels, test_size=0.2, random_state=42)\n",
    "    meta_model = LogisticRegression(class_weight='balanced')\n",
    "    meta_model.fit(X_train, y_train)\n",
    "    y_pred = meta_model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Meta-model accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return meta_model\n",
    "\n",
    "def predict_regression_ensemble(meta_model, models, tokenizers, dataset):\n",
    "    logits = get_model_predictions(models, tokenizers, dataset)\n",
    "    final_predictions = meta_model.predict(logits)\n",
    "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
    "    return [reverse_label_dict[x] for x in final_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [model1, model2, model3]\n",
    "tokenizers = [tokenizer1, tokenizer2, tokenizer3]\n",
    "ensemble = regression_ensemble(models, tokenizers, dataset_dict['train'], dataset_dict['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions = predict_regression_ensemble(ensemble, models, tokenizers, dataset_dict['validation'])\n",
    "print(len(validation_predictions))\n",
    "validation_predictions[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].remove_columns([\"predictions\"])\n",
    "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
    "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
    "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
    "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
    "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_regression_ensemble(ensemble, models, tokenizers, dataset_dict['test'])\n",
    "print(len(test_predictions))\n",
    "test_predictions[1925:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_ensemble_reg.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_ensemble_reg.csv'}, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на CodaLab\n",
    "\n",
    "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
    "|----------|----------|----------|\n",
    "| Ансамбль из трёх моделей  | 50.62     | 62.19   |"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
