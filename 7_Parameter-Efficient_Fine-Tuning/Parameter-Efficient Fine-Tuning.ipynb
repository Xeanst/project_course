{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из главных плюсов больших языковых моделей является возможность использовать их без дополнительного обучения. Зачастую решить необходимую задачу можно просто с помощью хорошо сформулированной инструкции. При наличии 10-50 примеров можно добавить их в инструкцию, чтобы лучше \"объяснить\", в чем состоит задача."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/Hn8zmRV7/prompt-engineering.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако может случиться, что задача слишком сложная и требует \"знаний\", которые не встречались при предобучении LLM. Другой пример — применение к языку, данные на котором отсутствовали в обучающем датасете.\n",
    "\n",
    "Тогда можно осуществить тоннкую настройку большой языковой модели (аналогично энкодерной модели). Важно, что для этого необходимо достаточное количество примеров.\n",
    "\n",
    "Проблема состоит в том, что дообучение всей LLM требует слишком много времени и ресурсов. Существуют различные методы, которые позволяют осуществлять тонкую настройку более эффективно и менять только часть весов модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/XNBt4zM8/Parameter-efficient-Fine-tuning.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT — это методы тонкой настройки, которые позволяют улучшить результаты работы языковых моделей при выполнении определенных задач. Их идея заключается в том, чтобы обучить небольшое подмножество параметров предобученной LLM, оставляя бо́льшую часть замороженными. Благодаря этому мы также можем избежать переобучения на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promt tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)\n",
    "\n",
    "Первый метод PEFT в некотором смысле является развитием идеи prompt engineering.\n",
    "\n",
    "Подбор инструкции вручную состоит в том, чтобы эмбеддинги входящих к нее слов при подаче в модель привели к желаемому результату генерации. Идея promt tuning состоит в том, чтобы избавиться от шага с подбором слов и сразу подбирать нужные эмбеддинги.\n",
    "\n",
    "Фиксированные инструкции (hard prompt) подбираются человеком и включают токены из словаря. Токены нефиксированной инструкции (soft prompt) являются обучаемыми векторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/SxGxVKDJ/hard-soft.jpg\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед входом в языковую модель размещается несколько обучаемых эмбеддингов.\n",
    "- У модели есть эмбеддинги токенов (input text), которые привязаны к осмысленным словам.\n",
    "- Также есть эмбеддинги, которые не зависят от слов, просто обучаемые векторы, причем каждый из них уникальный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/x8VwWvdY/image1.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы создать soft prompt для некоторой задачи, сначала инициализируется промпт в виде последовательности векторов фиксированной длины (например, длиной в 20 токенов). Эти векторы объединяются с входными примерами и подаются в модель.\n",
    "\n",
    "Предсказание модели сравнивается с целевым значением для расчета функции потерь, и её значение передается обратно для расчета градиентов. Однако обновляться будут только новые обучаемые векторы, тогда как базовая модель остается замороженной.\n",
    "\n",
    "Soft prompts, полученные таким образом, не поддаются человеческой интерпретации. На интуитивном уровне они содержат информацию о том, как наилучшим образом сформулировать задачу, имея размеченный набор данных. Они выполняют ту же роль, что и написанные вручную инструкции, но без необходимости сопоставлять эмбеддингам слова человеческого языка.\n",
    "\n",
    "Чем больше размер модели, тем лучше работает данный метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/SKNCf37b/image3.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)\n",
    "\n",
    "[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)\n",
    "\n",
    "Развитие идеи promt tuning: вместо обучения промпта для входного слоя обучать свои промпты для каждого слоя.\n",
    "\n",
    "Количество обучаемых параметров увеличивается в N раз, где N — количество слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Prompt tuning</em>\n",
    "<center><img src =\"https://i.postimg.cc/cHFDcf8X/prompt.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Prefix tuning</em>\n",
    "<center><img src =\"https://i.postimg.cc/zf50ZxCx/prefix.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Адаптеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)\n",
    "\n",
    "Вместо того чтобы обучать эмбеддинги дополнительных фиктивных токенов, предлагается добавить в архитектуру нейросети маленькие обучаемые слои — адаптеры.\n",
    "\n",
    "Добавляя их после слоев внимания (multi-head attention) и линейных слоев (feed-forward) в архитектуре Трансформер, мы можем обновлять только веса в адаптерах во время тонкой настройки, сохраняя при этом остальные параметры модели неизменными.\n",
    "\n",
    "Адаптер содержит два линейных слоя, между которыми находится нелинейная функция активации. Входными данными для слоя адаптера будет скрытое представление $h$, которое является выходом слоя множественного внимания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/ZnF6cSgZ/adapter.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При проходе через первый линейный слой адаптера вектор $h$ проецируется в пространство малой размерности. После этого применяется функция активации. Выход второго линейного слоя имеет ту же размерность, как у изначального вектора $h$. Вектор $\\Delta h$, полученный при проходе через адаптор, суммируется с исходным вектором $h$ (skip-connection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "Существуют адаптеры, которые работают на уровне трансформерного слоя, а на уровне весов. Идея состоит в том, чтобы модифицировать каждую матрицу слоя множественного внимания: `q_proj` (query), `k_proj` (key), `v_proj` (value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/7PFx1zMx/Transformer-architecture-in-wav2vec2-along-with-Lo-RA.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подробнее разобрать метод LoRA, спустимся на более простой уровень. Пусть у нас есть один линейный слой без функции активации.\n",
    "\n",
    "Если на вход подадим $x$, на выходе получим $y = Wx$ где $W$ — матрица весов.\n",
    "\n",
    "Мы хотим немного изменить принцип работы этого слоя, дообучив модель, скорректировав веса на $\\Delta W$ (которые ищут обычным градиентным спуском), так что бы новый выход был:\n",
    "\n",
    "$$y=W'x=(W+\\Delta W)x=y+\\Delta Wx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/SRkHVcdP/48f5024475f644b2d1cacbda2a8cb0b6.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы можем зафиксировать веса матрицы $W$, а вместо этого учить $\\Delta W$ — матрицу, предсказывающую отличие результата обычной модели, от дообученой.\n",
    "\n",
    "Сразу возникнет вопрос — а где тут выигрыш? Ведь размеры матриц $W$ и $\\Delta W$ должны быть одинаковыми, так что в них одинаковое количество обучаемых параметров.\n",
    "\n",
    "Вот тут и включаются в игру слова Low Rank — матрицу маленького ранга можно представить как произведение двух меньшей размерности. Наша матрица может быть размером 100 на 70, но ранг, то есть количество линейно независимых строк или столбцов (таких столбцов которые действительно содержат новую информацию о модели, а не действуют на вектор параметров аналогично соседям) может быть меньше, чем 70, — например 4 или 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим ненулевой вектор  $\\vec a=(1, -1, 2)$. Его ранг равен 1.\n",
    "\n",
    "Рассмотрим матрицу, строки которой линейно зависимы (выражаются друг через друга).\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "1 & -1 & 2\\\\\n",
    "2 & -2 & 4\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "С геометрической точки зрения во вторую строку записаны координаты коллинеарного вектора $2 \\vec a=(2, -2, 4)$. Таким образом, ранг данной матрицы тоже равен 1.\n",
    "\n",
    "Познакомимся с матрицей, строки которой линейно независимы.\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "1 & -1 & 2\\\\\n",
    "0 & 1 & -1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Пара векторов $\\vec a=(1, -1, 2)$ и $\\vec b=(0, 1, -1)$ не коллинеарны. Ран матрицы равен 2.\n",
    "\n",
    "Ранг матрицы – это максимальное количество линейно независимых строк. Или: ранг матрицы – это максимальное количество линейно независимых столбцов. Их количество всегда совпадает.\n",
    "\n",
    "Из вышесказанного также следует важный практический ориентир: ранг матрицы не превосходит её минимальной размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, можно представить матрицу $\\Delta W$ как произведение двух матриц $A$ и $B$. При этом сильно выиграем в количестве обучаемых параметров.\n",
    "\n",
    "Для примера на картинке матрица 100 х 70 содержит 7000 чисел, а две в левой части неравенства 140 + 200 = 340."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/25WJD0XJ/79d036c365cd35a10ae1c80cc3e5a2e1.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "В общем случае потребуется обучать в $\\frac{ nr + rn }{ n^2 } = \\frac{ 2r }{n}$ меньше параметров. $r$ выбирается маленьким, порядка 2-8, что делает это значение очень маленьким $\\approx 10^{-2}$.\n",
    "\n",
    "Мы немного потеряем в общности, так как теперь  автоматический постулируем, что у $\\Delta W$ низкий ранг. Однако в этом нет ничего страшного: разработчики LoRA утверждают что хотя LLM имеют миллионы или даже миллиарды параметров, они имеют низкую \"внутреннюю размерность\" (intrinsic dimension) при адаптации к новой задаче. Проще говоря, большинство параметров являются избыточными. Из чего можно сделать вывод, что матрицы можно представить пространством меньшей размерности, сохраняя при этом большую часть важной информации.\n",
    "\n",
    "[Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)\n",
    "\n",
    "Таким образом, во время обучения нам необходимо хранить в памяти веса $W$ исходной модели и $\\Delta W=B\\cdot A$ дообучаемой, а считать градиенты только для \"новых\" маленьких матриц $A$ и $B$.\n",
    "\n",
    "При инициализации модели мы создаем матрицу $B$ случайным образом, а матрицу $A$ инициализируем нулями, что бы изначально $\\Delta W = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/vHD0yP4z/252844b9dbfa3f1125a54997087dd2f5.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тонкая настройка LLM с помощью LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрим, как осуществить тонкую настройку больших языковых моделей с помощью метода LoRa (Low-Rank Adaptation), который:\n",
    "- Замораживает значения весов для предобученной модели\n",
    "- Добавляет небольшие обучаемые матрицы ранговой декомпозиции к слоям внимания\n",
    "- Обычно уменьшает количество обучаемых параметров примерно на 90%\n",
    "- Поддерживает производительность модели при сохранении эффективной работы с памятью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install transformers datasets trl huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Получаем токен из Secrets Colab\n",
    "hf_token = userdata.get('hf_token')\n",
    "\n",
    "# Авторизуемся в Hugging Face Hub\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) из библиотеки `trl` обеспечивает интеграцию с адаптерами LoRa через библиотеку [PEFT](https://huggingface.co/docs/peft/en/index).\n",
    "\n",
    "Основные преимущества включают в себя:\n",
    "\n",
    "**1. Эффективность использования памяти:**\n",
    "- В памяти графического процессора хранятся только параметры адаптера\n",
    "- Веса базовой модели остаются замороженными и могут быть загружены с меньшей точностью\n",
    "- Позволяет выполнять тонкую настройку больших моделей на потребительских графических процессорах\n",
    "\n",
    "**2. Особенности обучения:**\n",
    "- Встроенная интеграция PEFT/LoRa с минимальными настройками\n",
    "- Поддержка QLoRA (Quantized LoRa) для еще большей эффективности использования памяти\n",
    "\n",
    "**3. Управление адаптером:**\n",
    "- Сохранение весов адаптера во время контрольных точек (checkpoints)\n",
    "- Функции для присоединения адаптеров обратно к базовой модели\n",
    "\n",
    "Для тонкой настройки требуется всего несколько шагов:\n",
    "- Определить конфигурацию LoRa (rank, alpha, dropout)\n",
    "- Создать SFTTrainer с помощью PEFT config\n",
    "- Обучить и сохранить веса адаптера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных и базовой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет, который будем использовать для тонкой настройки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим предобученную модель и токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели и токенизаторы\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Определим шаблон чата\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Зададим директорию, куда будут сохраняться веса адаптора\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем предобученную модель для генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(prompt, pipe):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt, pipe)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тонкая настройка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFTTrainer` поддерживает интеграцию с библиотекой `peft`, что упрощает эффективную настройку LLM, например, с использованием LoRa. Нам нужно только создать `LoraConfig` и передать его в `trainer`.\n",
    "\n",
    "Определим параметры LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r: измерение ранга для матриц обновления LoRa (меньшее значение = большее сжатие)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: коэффициент масштабирования для слоев LoRa (чем выше, тем больше влияние на исходные веса)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: вероятность зануления слоев LoRa (помогает предотвратить переобучение)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # ранговый показатель - обычно от 4 до 32\n",
    "    lora_alpha=lora_alpha,  # коэффициент масштабирования LoRa - обычно 2х для ранга\n",
    "    lora_dropout=lora_dropout,  # вероятность зануления для слоев LoRa\n",
    "    target_modules=\"all-linear\",  # к каким модулям применять LoRa\n",
    "    task_type=\"CAUSAL_LM\",  # тип задачи для архитектуры модели\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем начать обучение, нам нужно определить гиперпараметры (`TrainingArguments`), которые будем использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    # Параметры сохранения\n",
    "    output_dir=finetune_name,\n",
    "    # Продолжительность обучения\n",
    "    num_train_epochs=1,\n",
    "    # Настройки размера батча\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # Оптимизация памяти\n",
    "    gradient_checkpointing=True,\n",
    "    # Настройки оптимизатора\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    # Планировщик скорости обучения\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # Логирование и сохранение\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    # Настройки точности\n",
    "    bf16=True,\n",
    "    # Настройки интеграции\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь есть все необходимые компоненты для создания `SFTTrainer`, чтобы начать тонкую настройку модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем SFTTrainer с конфигурацией LoRa\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем обучение модели, вызвав метод train() для экземпляра класса Trainer. Поскольку мы используем метод PEFT, сохранятся только адаптированные веса модели, а не полная модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# после начала обучения модель будет сохранена в выходной директории\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объединение адаптора LoRA с предобученной моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании LoRa мы обучаем только веса адаптеров, сохраняя базовую модель замороженной. Во время обучения мы сохраняем только эти веса адаптеров (~2-10 МБ), а не полную копию модели. Однако для развертывания может потребоваться объединить адаптеры обратно в базовую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка весов адаптора на CPU\n",
    "lora_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# объединение LoRA с базовой моделью и сохранение\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir,\n",
    "                             safe_serialization=True,\n",
    "                             max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем несколько примеров и посмотрим, как работает модель после тонкой настройки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt, merged_pipe)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ изменений для конкретного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходная модель имеет следующую архитектуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем матрицу весов Q (query) для первого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = model.model.layers[0].self_attn.q_proj.weight.data\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель после тонкой настройки имеет аналогичную структуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем ту же самую матрицу весов и убедимся, что она отличается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_model.model.layers[0].self_attn.q_proj.weight.data\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем в отдельные переменные низкоранговые матрицы A и B, которые обучались с помощью метода LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight.data\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = trainer.model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.default.weight.data\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что при их перемножении и прибавлении к матрице базовой модели получим такую же матрицу, как в модели после тонкой настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/438KX176/eddf88ff00409fa19ff21826a26e90d2.gif\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base + lora_alpha/rank_dimension * (b @ a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
