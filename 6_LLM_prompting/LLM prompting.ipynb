{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом блокноте мы рассмотрим некоторые практические аспекты использования больших языковых моделей, а именно доступность и подбор промтов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование LLM в Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько способов использовать большие языковые модели в среде Google Colab. Для всех них необходимы ключи доступа, которые можно хранить в разделе \"Секреты\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yandex Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Доступны](https://yandex.cloud/ru/docs/foundation-models/concepts/yandexgpt/models) модели серии YandexGPT и LLama.\n",
    "\n",
    "Плюсы: быстрая работа.\n",
    "\n",
    "Минусы: платное использование по завершении приветственного гранта в 4000 рублей.\n",
    "\n",
    "Цена за 1000 токенов:\n",
    "- YandexGPT Lite (`yandexgpt-lite`) — 0,20 ₽\n",
    "- YandexGPT Pro (`yandexgpt`) — 1,20 ₽\n",
    "- Llama 8b (`llama-lite`) — 0,20 ₽\n",
    "- Llama 70b (`llama`) — 1,20 ₽\n",
    "\n",
    "Для доступа необходимы API-ключ и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
    "yandex_gpt = userdata.get(\"yandex_gpt\")\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Api-Key {yandex_gpt}\" # ваш секретный ключ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "prompt = {\n",
    "    \"modelUri\": \"gpt://b1gu40ajd726f9s9h843/yandexgpt\", # идентификатор модели\n",
    "    \"completionOptions\": {\n",
    "        \"stream\": False, # отключение режима диалога\n",
    "        \"temperature\": 1 # степень рандомности генерации\n",
    "        },\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"text\": \"Что изучает лингвистика?\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "response = requests.post(url, headers=headers, json=prompt).text\n",
    "print(response)\n",
    "text = json.loads(response)[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yandexgpt(content):\n",
    "  prompt = {\n",
    "      \"modelUri\": \"gpt://b1gu40ajd726f9s9h843/yandexgpt\",\n",
    "      \"completionOptions\": {\n",
    "          \"stream\": False,\n",
    "          \"temperature\": 1\n",
    "          },\n",
    "      \"messages\": [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"text\": content\n",
    "              }\n",
    "          ]\n",
    "      }\n",
    "\n",
    "  response = requests.post(url, headers=headers, json=prompt).text\n",
    "  text = json.loads(response)[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = yandexgpt(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Доступны](https://developers.sber.ru/docs/ru/gigachat/api/tariffs) модели серии GigaChat: Lite (`GigaChat-Lite`), Pro (`GigaChat-Pro`) и Max (`GigaChat-Max`).\n",
    "\n",
    "Плюсы: быстрая работа.\n",
    "\n",
    "Минусы: платное использование по завершении приветственного гранта на 50 000 токенов для каждой модели.\n",
    "\n",
    "Тарификация:\n",
    "\n",
    "- 5 000 000 токенов GigaChat Lite — 1 000 ₽\n",
    "- 30 000 000 токенов GigaChat Lite — 5 820 ₽\n",
    "- 1 000 000 токенов GigaChat Pro — 1 500 ₽\n",
    "- 5 000 000 токенов GigaChat Pro — 7 275 ₽\n",
    "- 1 000 000 токенов GigaChat Max — 1 950 ₽\n",
    "- 4 000 000 токенов GigaChat Max — 7 566 ₽\n",
    "\n",
    "Для доступа необходимы идентификатор клиента (`RqUID`) и API-ключ (`Authorization`), которые позволяют получить токен доступа (`access_token`). Также нужно указать идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "\n",
    "payload={\n",
    "  'scope': 'GIGACHAT_API_PERS'\n",
    "}\n",
    "\n",
    "gc_RqUID = userdata.get(\"gc_RqUID\")\n",
    "gc_key = userdata.get(\"gc_key\")\n",
    "headers = {\n",
    "  'Content-Type': 'application/x-www-form-urlencoded',\n",
    "  'Accept': 'application/json',\n",
    "  'RqUID': gc_RqUID,\n",
    "  'Authorization': f'Basic {gc_key}'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=payload, verify=False)\n",
    "token = response.json().get('access_token')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + token,\n",
    "    'Content-Type': 'application/json',\n",
    "    }\n",
    "payload=json.dumps({\n",
    "    \"model\": \"GigaChat-Pro\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Что изучает лингвистика?\"\n",
    "            }\n",
    "        ],\n",
    "    \"temperature\": 1,\n",
    "    \"stream\": False,\n",
    "    })\n",
    "response = requests.post(url, headers=headers, data=payload, verify=False)\n",
    "print(response.text)\n",
    "text = response.json()['choices'][0]['message']['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gigachat(content):\n",
    "  url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
    "  headers = {\n",
    "      'Authorization': f'Bearer {token}',\n",
    "      'Content-Type': 'application/json',\n",
    "      }\n",
    "  payload=json.dumps({\n",
    "      \"model\": \"GigaChat-Pro\",\n",
    "      \"messages\": [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "              }\n",
    "          ],\n",
    "      \"temperature\": 1,\n",
    "      \"stream\": False,\n",
    "      })\n",
    "  response = requests.post(url, headers=headers, data=payload, verify=False)\n",
    "  text = response.json()['choices'][0]['message']['content']\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = gigachat(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Доступны](https://docs.mistral.ai/getting-started/models/models_overview/) модели серии Mistral:\n",
    "- мультиязычные LLM\n",
    "  - Mistral Large (`mistral-large-latest`)\n",
    "  - Ministral 8B (`ministral-8b-latest`)\n",
    "  - Ministral 3B\t(`ministral-3b-latest`)\n",
    "- LLM для языков Ближнего Востока и Южной Азии\n",
    "  - Mistral Saba (`mistral-saba-latest`)\n",
    "- LLM для кода\n",
    "  - Codestral (`codestral-latest`)\n",
    "- мультимодальная LLM\n",
    "  - Pixtral Large (`pixtral-large-latest`)\n",
    "\n",
    "Плюсы: бесплатное использование.\n",
    "\n",
    "Минусы: долгая работа.\n",
    "\n",
    "Для доступа необходимы API-ключ и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "\n",
    "mistral = userdata.get(\"mistral\")\n",
    "headers = {\n",
    "      'Authorization': f'Bearer {mistral}',\n",
    "      'Content-Type': 'application/json',\n",
    "      }\n",
    "payload = {\n",
    "    \"model\": \"mistral-large-latest\",\n",
    "    \"temperature\": 1,\n",
    "    \"stream\": False,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Что изучает лингвистика?\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "response = requests.post(\"https://api.mistral.ai/v1/chat/completions\", headers=headers, json=payload)\n",
    "print(response.json())\n",
    "text = response.json()['choices'][0]['message']['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral(content):\n",
    "  mistral = userdata.get(\"mistral\")\n",
    "  headers = {\n",
    "        'Authorization': f'Bearer {mistral}',\n",
    "        'Content-Type': 'application/json',\n",
    "        }\n",
    "  payload = {\n",
    "      \"model\": \"mistral-large-latest\",\n",
    "      \"temperature\": 1,\n",
    "      \"stream\": False,\n",
    "      \"messages\": [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "              }\n",
    "          ]\n",
    "      }\n",
    "  response = requests.post(\"https://api.mistral.ai/v1/chat/completions\", headers=headers, json=payload)\n",
    "  text = response.json()['choices'][0]['message']['content']\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = mistral(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Доступно](https://api.together.ai/models) достаточно много разных моделей по разным тарифам:\n",
    "- DeepSeek R1\n",
    "- Qwen 2.5 Instruct\n",
    "- Gemma Instruct\n",
    "\n",
    "Однако есть две бесплатные модели:\n",
    "- Meta Llama 3.3 70B Instruct Turbo Free (`meta-llama/Llama-3.3-70B-Instruct-Turbo-Free`)\n",
    "- DeepSeek R1 Distill Llama 70B Free (`deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free`)\n",
    "\n",
    "Плюсы: возможно бесплатное использование.\n",
    "\n",
    "Минусы: долгая работа.\n",
    "\n",
    "Для доступа необходимы API-ключ и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import openai\n",
    "\n",
    "together_ai = userdata.get(\"together_ai\")\n",
    "client = openai.OpenAI(\n",
    "    api_key=together_ai,\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    )\n",
    "response = client.chat.completions.create(\n",
    "    model= \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    temperature=1,\n",
    "    stream=False,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Что изучает лингвистика?\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "print(response)\n",
    "text = response.choices[0].message.content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama(content):\n",
    "  together_ai = userdata.get(\"together_ai\")\n",
    "  client = openai.OpenAI(\n",
    "      api_key=together_ai,\n",
    "      base_url=\"https://api.together.xyz/v1\",\n",
    "      )\n",
    "  response = client.chat.completions.create(\n",
    "      model= \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "      temperature=1,\n",
    "      stream=False,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "              }\n",
    "          ]\n",
    "      )\n",
    "  text = response.choices[0].message.content\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = llama(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На Hugging Face [доступно](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) огромное количество моделей. Модели, дообученные с помощью SFT, обозначены как Instruct.\n",
    "\n",
    "Возможность использования графического процессора в Google Colab позволяет напрямую импортировать модели до 2 миллиардов параметров.\n",
    "\n",
    "Плюсы: бесплатное использование.\n",
    "\n",
    "Минусы: долгая работа, ограничения на размер модели.\n",
    "\n",
    "Для доступа необходимы GPU и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "qwen_1_5_b = pipeline(\"text-generation\",\n",
    "                      model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                      temperature=1,\n",
    "                      max_new_tokens=1000,\n",
    "                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Что изучает лингвистика?\"\n",
    "        },\n",
    "]\n",
    "response = qwen_1_5_b(messages)\n",
    "print(response)\n",
    "text = response[0][\"generated_text\"][1][\"content\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen(content):\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": content\n",
    "          },\n",
    "      ]\n",
    "  response = qwen_1_5_b(messages)\n",
    "  text = response[0][\"generated_text\"][1][\"content\"]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = qwen(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели большего размера доступны через [InferenceClient](https://huggingface.co/docs/huggingface_hub/v0.16.2/en/package_reference/inference_client#huggingface_hub.InferenceClient).\n",
    "\n",
    "Плюсы: бесплатное использование.\n",
    "\n",
    "Минусы: ограничение на 1000 запросов в сутки, долгая работа.\n",
    "\n",
    "Для доступа необходимы токен и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hf_token = userdata.get(\"hf_token\")\n",
    "model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "client = InferenceClient(model_name, token=hf_token)\n",
    "\n",
    "output = client.chat.completions.create(\n",
    "          messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": \"Что изучает лингвистика?\"\n",
    "                  },\n",
    "              ],\n",
    "          max_tokens=1000,\n",
    "          temperature=1)\n",
    "print(output)\n",
    "text = output.choices[0].get('message')['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_72(content):\n",
    "  hf_token = userdata.get(\"hf_token\")\n",
    "  model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "  client = InferenceClient(model_name, token=hf_token)\n",
    "\n",
    "  output = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "              },\n",
    "          ],\n",
    "      max_tokens=1000,\n",
    "      temperature=1)\n",
    "  text = output.choices[0].get('message')['content']\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for question in questions:\n",
    "  answer = qwen_72(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroqCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Доступны](https://console.groq.com/docs/rate-limits) различные модели (сайт открывается через VPN):\n",
    "- `llama-3.3-70b-versatile`\n",
    "- `qwen-2.5-32b`\n",
    "- `deepseek-r1-distill-qwen-32b`\n",
    "- `deepseek-r1-distill-llama-70b`\n",
    "\n",
    "Плюсы: бесплатное использование, быстрая работа.\n",
    "\n",
    "Минусы: ограничение на количество запросов (для каждой модели отличается).\n",
    "\n",
    "Для доступа необходимы API-ключ и идентификатор модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from groq import Groq\n",
    "\n",
    "groq = userdata.get(\"groq\")\n",
    "client = Groq(\n",
    "    api_key=groq,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Что изучает лингвистика?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "print(chat_completion)\n",
    "text = chat_completion.choices[0].message.content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek(content):\n",
    "\n",
    "  groq = userdata.get(\"groq\")\n",
    "  client = Groq(\n",
    "      api_key=groq,\n",
    "  )\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": content\n",
    "              }\n",
    "          ],\n",
    "      model=\"llama-3.3-70b-versatile\",\n",
    "      temperature=1\n",
    "      )\n",
    "\n",
    "  text = chat_completion.choices[0].message.content\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "questions = [\"Что изучает лингвистика?\",\n",
    "             \"Чем отличаются фонетика и фонология?\",\n",
    "             \"Почему языки такие разные?\"]\n",
    "\n",
    "for question in questions:\n",
    "  answer = deepseek(question)\n",
    "  print(f\"Вопрос:\\n{question}\")\n",
    "  print(f\"Ответ:\\n{answer}\\n\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затравочное программирование (prompt engineering) — разработка и оптимизация затравок (промптов) для эффективного использования больших языковых моделей.\n",
    "\n",
    "Рассмотрим различные способы формулировки промптов на примере модуля ChatGroq библиотеки LangChain. Он позволяет использовать модели из GroqCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_groq -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовое использование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для инициализации модели необходим API-ключ и ее идентификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq = userdata.get(\"groq\")\n",
    "llm = ChatGroq(\n",
    "    temperature=1,\n",
    "    groq_api_key = groq,\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"Что изучает лингвистика?\"\n",
    "print(llm.invoke(basic_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от конкретной формулировки промпта ответ может различаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Перечисли 4 примера применения искусственного интеллекта в здравоохранении\",\n",
    "    \"Объясни, как искусственный интеллект меняет область здравоохранения на 4 конкретных примерах\",\n",
    "    \"Ты врач. Опиши 4 способа, с помощью которых искусственный интеллект улучшил твою повседневную работу в больнице\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "  print(f\"\\nПромпт {i}: \")\n",
    "  print(prompt)\n",
    "  print(\"\\nОтвет: \")\n",
    "  print(llm.invoke(prompt).content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структурированный промпт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модуль `PromptTemplate` позволяет использовать переменные внутри промпта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template = \"Что изучает {topic}?\"\n",
    ")\n",
    "chain = structured_prompt | llm\n",
    "input_variables = {\"topic\": \"лингвистика\"}\n",
    "output = chain.invoke(input_variables).content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_check_prompt = PromptTemplate(\n",
    "    input_variables=[\"statement\"],\n",
    "    template=\"\"\"Оцени приведенное ниже утверждение на предмет достоверности. Если оно неверно, укажи правильную информацию:\n",
    "    Утверждение: {statement}\n",
    "    Оценка:\"\"\"\n",
    ")\n",
    "\n",
    "input_variables = {\"statement\": \"Столицей Индии является Лондон.\"}\n",
    "chain = fact_check_prompt | llm\n",
    "print(chain.invoke(input_variables).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_solving_prompt = PromptTemplate(\n",
    "    input_variables=[\"problem\"],\n",
    "    template = \"\"\"Реши следующую задачу шаг за шагом:\n",
    "    Задача: {problem}\n",
    "    Решение:\n",
    "    1)\"\"\"\n",
    ")\n",
    "\n",
    "input_variables = {\"problem\": \"Банк предлагает годовую процентную ставку в размере 6%, которая ежегодно увеличивается. \\\n",
    "За последние 5 лет суммы годовых депозитов были следующими: $1000, $1500, $2000, $2500, и $3000 долларов США. \\\n",
    "Рассчитайте общую сумму на счете по истечении 5 лет с учетом ежегодного начисления процентов.\"}\n",
    "chain = problem_solving_prompt | llm\n",
    "print(chain.invoke(input_variables).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последовательность реплик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ведения диалога более чем из 1 реплики используется модуль ConversationChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    memory = ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=\"Что такое галактика?\"))\n",
    "print(conversation.predict(input=\"Сколько галактик во Вселенной?\"))\n",
    "print(conversation.predict(input=\"Как называется галактика, в которой мы живем?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это помогает запоминать предыдущий контекст диалога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Какой город является столицей Франции?\",\n",
    "    \"Каково его население?\",\n",
    "    \"Какая самая известная достопримечательность этого города?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {llm.invoke(prompt).content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
    "for prompt in prompts:\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {conversation.predict(input=prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Режим zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При достаточно подробной формулировке инструкции задача может быть решена без каких-либо демонстрационных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt_template):\n",
    "  prompt = PromptTemplate.from_template(prompt_template)\n",
    "  return prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_task_prompt = \"\"\"Классифицируй тональность следующего текста как положительную, отрицательную или нейтральную. \\\n",
    "Не объясняй свои доводы, просто приведи классификацию.\n",
    "\n",
    "Текст: {text}\n",
    "Тональность: \"\"\"\n",
    "\n",
    "direct_task_chain = create_chain(direct_task_prompt)\n",
    "\n",
    "texts = [\n",
    "    \"В новом кафе в городе такая уютная атмосфера, и кофе превосходный!\",\n",
    "    \"Книга была неплохой, но я бы не сказал, что она выделялась на фоне других, которые я читал.\",\n",
    "    \"Опыт онлайн-покупок был разочаровывающим; веб-сайт постоянно зависал.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "  result = direct_task_chain.invoke({\"text\": text}).content\n",
    "  print(f\"Текст: {text}\")\n",
    "  print(f\"Тональность: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определенных задач важно детально задать формат ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_spec_prompt = \"\"\"Создай короткую новостную статью по теме \"{topic}\".\n",
    "Структурируй свой ответ в следующем формате:\n",
    "\n",
    "Заголовок: [Броский заголовок для статьи]\n",
    "\n",
    "Введение: [Краткий вводный абзац, в котором излагаются ключевые моменты]\n",
    "\n",
    "Основное содержание: [2-3 коротких абзаца с более подробной информацией]\n",
    "\n",
    "Вывод: [Заключительное предложение или призыв к действию]\"\"\"\n",
    "\n",
    "format_spec_chain = create_chain(format_spec_prompt)\n",
    "\n",
    "topic = \"Прорыв в технологии хранения возобновляемой энергии\"\n",
    "result = format_spec_chain.invoke({\"topic\": topic}).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, можно задать конкретные этапы выполнения задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_prompt = \"\"\"Проанализируй следующий текст на предмет его основного аргумента, подтверждающих доказательств и потенциальных контраргументов.\n",
    "Проведи свой анализ по следующим этапам:\n",
    "\n",
    "1. Главный аргумент: Определи и сформулируй основное утверждение или тезис.\n",
    "2. Подтверждающие доказательства: Перечисли ключевые моменты или доказательства, используемые в поддержку основного аргумента.\n",
    "3. Возможные контраргументы: Предложите возможные возражения или альтернативные точки зрения на основной аргумент.\n",
    "\n",
    "Текст: {text}\n",
    "\n",
    "Анализ:\"\"\"\n",
    "\n",
    "multi_step_chain = create_chain(multi_step_prompt)\n",
    "\n",
    "text = \"\"\"В последние годы удаленная работа становится все более популярной, предлагая многочисленные преимущества как сотрудникам, так и работодателям.\n",
    "Работники пользуются большей гибкостью, сокращают время на дорогу и могут создавать более персонализированную рабочую среду.\n",
    "Работодатели выигрывают от снижения расходов на офис и доступа к более широкому кадровому резерву.\n",
    "Однако такие проблемы, как поддержание совместной работы в команде, управление производительностью и обеспечение безопасности данных по-прежнему сохраняются, что делает переход на удаленную работу не лишенным недостатков.\"\"\"\n",
    "\n",
    "result = multi_step_chain.invoke({\"text\": text}).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Режим few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление примеров помогает добиться лучшего решения задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_sentiment_classification(input_text):\n",
    "  few_shot_prompt = PromptTemplate(\n",
    "      input_variables=[\"input_text\"],\n",
    "      template=\"\"\"\n",
    "      Классифицируй тональность как положительную, отрицательную или нейтральную.\n",
    "\n",
    "      Пример:\n",
    "      Текст: Мне нравится этот продукт! Он потрясающий.\n",
    "      Тональность: Положительная\n",
    "      Текст: Этот фильм был ужасен. Я его возненавидел.\n",
    "      Тональность: Отрицательная\n",
    "      Текст: Погода сегодня неплохая.\n",
    "      Тональность: Нейтральная\n",
    "\n",
    "      Теперь классифицируй следующее предложение\n",
    "      Текст: {input_text}\n",
    "      Тональность:\n",
    "      \"\"\"\n",
    "  )\n",
    "  chain = few_shot_prompt | llm\n",
    "  result = chain.invoke(input_text).content\n",
    "  result = result.strip()\n",
    "\n",
    "  if ':' in result:\n",
    "    result = result.split(':')[1].strip()\n",
    "\n",
    "  return result\n",
    "\n",
    "test_text = \"Я не могу поверить, насколько велик и духовен кедарнатх!\"\n",
    "\n",
    "result = few_shot_sentiment_classification(test_text)\n",
    "print(f\"Текст : {test_text}\")\n",
    "print(f\"Тональность: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_task_few_shot(input_text, task):\n",
    "    few_shot_prompt = PromptTemplate(\n",
    "        input_variables=[\"input_text\", \"task\"],\n",
    "        template=\"\"\"\n",
    "        Выполни указанное задание по данному тексту.\n",
    "\n",
    "        Примеры:\n",
    "        Текст: Мне нравится этот продукт! Он потрясающий.\n",
    "        Задание: тональность\n",
    "        Результат: положительная\n",
    "\n",
    "        Текст: Это самый худший опыт, который у меня когда-либо был.\n",
    "        Задание: тональность\n",
    "        Результат: отрицательная\n",
    "\n",
    "        Текст: Bonjour, comment allez-vous?\n",
    "        Задание: язык\n",
    "        Результат: французский\n",
    "\n",
    "        Текст: Guten Tag, wie geht es Ihnen?\n",
    "        Задание: язык\n",
    "        Результат: немецкий\n",
    "\n",
    "        Текст: কেমন আছেন? (Kemon achhen?)\n",
    "        Задание: язык\n",
    "        Результат: бенгальский\n",
    "\n",
    "        Текст: От топота копыт пыль по полю летит.\n",
    "        Задание: подсчет слов\n",
    "        Результат: 7\n",
    "\n",
    "        Теперь выполни следующее задание:\n",
    "        Текст: {input_text}\n",
    "        Задание: {task}\n",
    "        Результат:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = few_shot_prompt | llm\n",
    "    return chain.invoke({\"input_text\": input_text, \"task\": task}).content\n",
    "\n",
    "print(multi_task_few_shot(\"Я не могу поверить, насколько это здорово!\", \"тональность\"))\n",
    "print(multi_task_few_shot(\"Guten Tag, wie geht es Ihnen?\", \"язык\"))\n",
    "print(multi_task_few_shot(\"কেমন আছেন?\", \"язык\"))\n",
    "print(multi_task_few_shot(\"Бык тупогуб, тупогубенький бычок, у быка бела губа была тупа.\", \"подсчет слов\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_context_learning(task_description, examples, input_text):\n",
    "    example_text = \"\".join([f\"Ввод: {e['input']}\\nВывод: {e['output']}\\n\\n\" for e in examples])\n",
    "\n",
    "    in_context_prompt = PromptTemplate(\n",
    "        input_variables=[\"task_description\", \"examples\", \"input_text\"],\n",
    "        template=\"\"\"\n",
    "        Задание: {task_description}\n",
    "\n",
    "        Примеры:\n",
    "        {examples}\n",
    "\n",
    "        Теперь выполни задание со следующими входными данными:\n",
    "        Ввод: {input_text}\n",
    "        Вывод:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = in_context_prompt | llm\n",
    "    return chain.invoke({\"task_description\": task_description, \"examples\": example_text, \"input_text\": input_text}).content\n",
    "\n",
    "task_desc = \"Преобразуй данный текст.\"\n",
    "examples = [\n",
    "    {\"input\": \"hello\", \"output\": \"ellohay\"},\n",
    "    {\"input\": \"apple\", \"output\": \"appleay\"}\n",
    "]\n",
    "test_input = \"python\"\n",
    "\n",
    "result = in_context_learning(task_desc, examples, test_input)\n",
    "print(f\"Ввод: {test_input}\")\n",
    "print(f\"Вывод: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цепочка размышлений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Промтинг с помощью цепочки размышлений (Chain-of-Thoughts, CoT), представленный в работе [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), позволяет LLM выполнять сложные задачи, требующие промежуточных шагов рассуждения. На популярном [бенчмарке](https://habr.com/ru/articles/840530/) по школьной арифметике GSM8K данный метод улучшает результат вдвое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.postimg.cc/J0q1n7LZ/CoT.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартный промпт\n",
    "standard_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Кратко ответь на следующий вопрос: {question}.\"\n",
    ")\n",
    "\n",
    "# Промпт с цепочкой рассуждений\n",
    "cot_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Кратко ответь на следующий вопрос шаг за шагом: {question}\"\n",
    ")\n",
    "\n",
    "standard_chain = standard_prompt | llm\n",
    "cot_chain = cot_prompt | llm\n",
    "\n",
    "# Пример вопроса\n",
    "question = \"Прямоугольник имеет длину 10 см и ширину 5 см. Какова его площадь в квадратных сантиметрах?\"\n",
    "\n",
    "standard_response = standard_chain.invoke(question).content\n",
    "cot_response = cot_chain.invoke(question).content\n",
    "\n",
    "print(\"Стандартный ответ:\")\n",
    "print(standard_response)\n",
    "print(\"\\nОтвет CoT: \")\n",
    "print(cot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_cot_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Реши следующую задачу шаг за шагом. Для каждого шага:\n",
    "1. Укажи, что ты собираешься рассчитать\n",
    "2. Напиши формулу, которую будешь использовать (если применимо).\n",
    "3. Выполни расчет\n",
    "4. Объясни результат\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Решение:\"\"\"\n",
    ")\n",
    "\n",
    "advanced_cot_chain = advanced_cot_prompt | llm\n",
    "\n",
    "complex_question = \"Цилиндрический резервуар имеет радиус 5 метров и высоту 10 метров. Сколько воды в кубических метрах он может вместить, если наполнить его на 80%?\"\n",
    "\n",
    "advanced_cot_response = advanced_cot_chain.invoke(complex_question).content\n",
    "print(advanced_cot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самосогласованность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из продвинутых техник для создания промтов является самосогласованность (self-consistency). Эта техника была предложена в работе [Wang et al. (2022)](https://arxiv.org/abs/2203.11171) в качестве замены «жадного» декодирования, используемого в цепочках рассуждений (CoT). Идея заключается в том, чтобы сэмплировать несколько разнообразных путей рассуждений через Few-shot CoT и использовать эти генерации для выбора наиболее согласованного ответа. Это помогает улучшить производительность CoT-промтов в задачах, связанных с арифметическими и логическими рассуждениями.\n",
    "\n",
    "Вначале сгенерируем несколько разных цепочек размышлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_paths(problem, num_paths=3):\n",
    "  prompt_template = PromptTemplate(\n",
    "      input_variables=[\"problem\" , \"path_number\"],\n",
    "      template=\"\"\"Решите следующую задачу, каждый раз используя уникальный подход. Это способ рассуждения номер {path_number}.\n",
    "      Задача: {problem}\n",
    "      Способ рассуждения {path_number}:\"\"\"\n",
    "  )\n",
    "  paths = []\n",
    "  for i in range(num_paths):\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"problem\": problem, \"path_number\": i+1}).content\n",
    "    paths.append(response)\n",
    "  return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Если поезд движется со скоростью 60 км/ч, сколько времени потребуется, чтобы преодолеть 180 км?\"\n",
    "paths = generate_multiple_paths(problem)\n",
    "\n",
    "for i, path in enumerate(paths, 1):\n",
    "  print(f\"Способ {i}: \\n{path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Следующим шагом самосогласованности будет агрегация и анализ ответов, направленный на выявление самого оптимального, который и станет окончательным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(paths):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"paths\"],\n",
    "        template=\"\"\"Проанализируйте приведенные ниже рассуждения и найдите наиболее логичный ответ. Если есть расхождения, объясните, почему, и укажите наиболее вероятный правильный ответ.\n",
    "        Способы рассуждения:\n",
    "        {paths}\n",
    "\n",
    "        Наиболее последовательный ответ:\"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"paths\": \"\\n\".join(paths)}).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = aggregate_results(paths)\n",
    "print(\"Итоговый результат: \\n\", aggregated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проводить сравнение ответов по конкретным заданным критериям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_check(problem, aggregated_result):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"problem\", \"result\"],\n",
    "        template=\"\"\"Оцените согласованность и надежность следующего результата для данной задачи.\n",
    "        Задача: {problem}\n",
    "        Результат: {result}\n",
    "\n",
    "        Оценка (учитывай такие факторы, как логическая последовательность, соответствие известным фактам и потенциальные предубеждения):\"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"problem\": problem, \"result\": aggregated_result}).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_evaluation = self_consistency_check(problem, aggregate_results)\n",
    "print(\"Оценка с помощью самосогласованности: \\n\", consistency_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично можно решать различные задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(problem):\n",
    "    paths = generate_multiple_paths(problem)\n",
    "    aggregated_result = aggregate_results(paths)\n",
    "    consistency_evaluation = self_consistency_check(problem, aggregated_result)\n",
    "    return aggregated_result, consistency_evaluation\n",
    "\n",
    "# Примеры задач\n",
    "problems = [\n",
    "    \"Какой город является столицей Франции?\",\n",
    "    \"Объясни концепцию спроса и предложения в экономике.\",\n",
    "    \"Если поезд движется со скоростью 70 км/ч, сколько времени потребуется, чтобы преодолеть 180 км?\"\n",
    "]\n",
    "\n",
    "for problem in problems:\n",
    "    print(f\"Задача: {problem}\")\n",
    "    result, evaluation = solve_problem(problem)\n",
    "    print(\"Итоговый результат:\\n\", result)\n",
    "    print(\"\\nОценка согласованности:\\n\", evaluation)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка роли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Промпт может содержать указание, поведение какого специалиста должна имитировать LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_writer_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template = \"\"\"Ты технический писатель, специализирующийся на создании понятной и сжатой документации к программному продукту.\n",
    "    Твоя задача — написать краткое объяснение темы {topic} для руководства пользователя.\n",
    "    Пожалуйста, предоставь объяснение в 2-3 предложениях, которое будет легко понятно пользователям, не имеющим технических знаний\"\"\"\n",
    ")\n",
    "chain = tech_writer_prompt | llm\n",
    "response = chain.invoke({\"topic\": \"Машинное обучение\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_advisor_prompt = PromptTemplate(\n",
    "    input_variables=[\"client_situation\"],\n",
    "    template=\"\"\"Вы опытный финансовый консультант с более чем 20-летним опытом работы в области личных финансов, инвестиционных стратегий и пенсионного планирования.\n",
    "    У вас есть опыт оказания помощи клиентам из разных слоев общества в достижении их финансовых целей.\n",
    "    Ваш подход характеризуется следующими принципами:\n",
    "    1. Тщательный анализ уникальной финансовой ситуации каждого клиента\n",
    "    2. Четкое изложение сложных финансовых концепций без использования жаргона\n",
    "    3. Соблюдение этических норм во всех рекомендациях\n",
    "    4. Акцент на долгосрочном финансовом благополучии и стабильности\n",
    "\n",
    "    Учитывая следующую ситуацию с клиентом, предоставьте краткую (3-4 предложения) финансовую консультацию:\n",
    "    {client_situation}\n",
    "\n",
    "    Ваш ответ должен отражать ваш опыт и соответствовать вашему характерному подходу.\"\"\"\n",
    ")\n",
    "\n",
    "chain = financial_advisor_prompt | llm\n",
    "response = chain.invoke({\"client_situation\": \"35-летний специалист, зарабатывающий 800 000 рублей в год, имеющий сбережения в размере 300 000 рублей, без долгов и пенсионного плана.\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение одних и тех же понятий может отличаться для разных ролей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = [\n",
    "    (\"Ученый\", \"Вы ученый-исследователь, специализирующийся на изменении климата. Объясните следующую концепцию в научных терминах:\"),\n",
    "    (\"Учитель\", \"Вы учитель естествознания в средней школе. Объясните следующую концепцию простыми словами, подходящими для 12-летних учеников:\"),\n",
    "    (\"Журналист\", \"Вы журналист, пишущий для научно-популярного журнала. Объясните следующую концепцию в увлекательной и информативной форме для широкой взрослой аудитории:\")\n",
    "]\n",
    "\n",
    "topic = \"парниковый эффект\"\n",
    "\n",
    "for role, description in roles:\n",
    "    role_prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=f\"{description} {{topic}}\"\n",
    "    )\n",
    "    chain = role_prompt | llm\n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    print(f\"\\nОбъясняет {role}:\\n\")\n",
    "    print(response.content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyteller_prompt = PromptTemplate(\n",
    "    input_variables=[\"style\", \"scenario\"],\n",
    "    template=\"\"\"Вы прекрасный рассказчик, известный своей способностью адаптироваться к различным стилям повествования.\n",
    "    Ваша текущая задача - писать в стиле {style}.\n",
    "    Ключевые характеристики этого стиля включают:\n",
    "    1. {style_char1}\n",
    "    2. {style_char2}\n",
    "    3. {style_char3}\n",
    "\n",
    "    Напишите короткий абзац (3-4 предложения) в таком стиле о следующем сценарии:\n",
    "    {scenario}\n",
    "\n",
    "    Убедитесь, что ваш текст четко соответствует указанному стилю.\"\"\"\n",
    ")\n",
    "\n",
    "styles = [\n",
    "    {\n",
    "        \"name\": \"Готический хоррор\",\n",
    "        \"char1\": \"Атмосферные и зловещие описания\",\n",
    "        \"char2\": \"Темы разложения, смерти и сверхъестественного\",\n",
    "        \"char3\": \"Обостренные эмоции и чувство страха\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Минималистский реализм\",\n",
    "        \"char1\": \"Скупой, лаконичный язык\",\n",
    "        \"char2\": \"Фокус на повседневных, заурядных событиях\",\n",
    "        \"char3\": \"Тонкие намеки, а не явные заявления\"\n",
    "    }\n",
    "]\n",
    "\n",
    "scenario = \"Человек входит в пустой дом в сумерках\"\n",
    "\n",
    "for style in styles:\n",
    "    chain = storyteller_prompt | llm\n",
    "    response = chain.invoke({\n",
    "        \"style\": style[\"name\"],\n",
    "        \"style_char1\": style[\"char1\"],\n",
    "        \"style_char2\": style[\"char2\"],\n",
    "        \"style_char3\": style[\"char3\"],\n",
    "        \"scenario\": scenario\n",
    "    })\n",
    "    print(f\"\\n{style['name']}:\\n\")\n",
    "    print(response.content)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
