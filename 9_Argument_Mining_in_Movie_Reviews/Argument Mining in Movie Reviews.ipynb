{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка к работе с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем исходный датасет blinoff/kinopoisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/blinoff/kinopoisk/kinopoisk.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор предложений для разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средний объем тренировочного датасета из соревнования RuArg-2022 -- 2296. Помимо тренировочной выборки, которая стандартно составляет 70% от общего датасета, также нужны валидационная и тестовая, включающих в себя по 15% от общего количества данных.\n",
    "Таким образом, нам нужно около 3280 текстов (2296 - 492 - 492).\n",
    "В целях экономии ресурсов, выберем самые короткие. В датасете blinoff/kinopoisk подходящая длина отзыва -- 93 слова или меньше (суммарно 3242 отзыва).\n",
    "Отфильтруем нужные строки и проанализируем получившийся набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  row[\"content\"] = row[\"content\"].replace('\\n', '')\n",
    "  row[\"content\"] = unicodedata.normalize(\"NFKD\", row[\"content\"])\n",
    "  if len(row[\"content\"].split()) <= 93:\n",
    "    short_texts.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_texts = pd.DataFrame(short_texts)\n",
    "\n",
    "print(len(short_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим количество отзывов в каждом классе (Good, Neutral, Bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_counts = short_texts['grade3'].value_counts()\n",
    "grade_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2199/3242)\n",
    "print(724/3242)\n",
    "print(319/3242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, классы не сбалансированы, что может повлиять на итоговые результаты.\n",
    "Рассмотрим отзывы длиной до 156 слов и выберем из них около 3280 с нужным соотношением (порядка 1100 отзывов каждого типа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  row[\"content\"] = row[\"content\"].replace('\\n', '') #в этой и следующей строчке -- предобработка текстов\n",
    "  row[\"content\"] = unicodedata.normalize(\"NFKD\", row[\"content\"])\n",
    "  if len(row[\"content\"].split()) <= 156:\n",
    "    short_texts.append(row)\n",
    "\n",
    "short_texts = pd.DataFrame(short_texts)\n",
    "print(len(short_texts))\n",
    "\n",
    "grade_counts = short_texts['grade3'].value_counts()\n",
    "grade_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество отрицательных отзывов - 1084, в двух других классах значительно больше, поэтому выберём 1098 нейтральных и 1098 положительных отзывов случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_bad = short_texts[short_texts[\"grade3\"] == \"Bad\"]\n",
    "\n",
    "short_neut = short_texts[short_texts[\"grade3\"] == \"Neutral\"].sample(1098)\n",
    "\n",
    "short_good = short_texts[short_texts[\"grade3\"] == \"Good\"].sample(1098)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем 3 класса отзывов в единый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([short_bad, short_neut, short_good], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))\n",
    "grade_counts = result['grade3'].value_counts()\n",
    "grade_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, отзывы сгруппированы по фильмам, поэтому перемешаем их случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экспортируем данные в формат Excel для дальнейшей ручной разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"blinoff.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание итогового размеченного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем размеченные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"processed_data_final.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Старые отзывы были автоматически отнесены к нейтральным при переходе на новую троичную систему. Также в отдельных случаях содержание отзывов и их категория не совпадала. Подобные несоответствия помечались разметчиками в столбце NEW_grade3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним NEW_grade3 для остальных отзывов\n",
    "\n",
    "dataframe['NEW_grade3'] = dataframe['NEW_grade3'].fillna(0)\n",
    "dataframe[\"NEW_grade3\"] = np.where(dataframe[\"NEW_grade3\"] == 0, dataframe[\"grade3\"], dataframe[\"NEW_grade3\"]) #берём оценку из исходного столбца\n",
    "print(dataframe[\"NEW_grade3\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем класс для аргументов на основе размеченного количества и в соответствии со следующими правилами:\n",
    "1.   В каждом отзыве подсчитывалось количество аргументов «за» и количество аргументов «против»\n",
    "2.   На основании полученных чисел выставлялся лейбл:\n",
    "\n",
    "*   если в обеих колонках «0» — Neutral\n",
    "*   если в одной колонке «0» — лейбл, соответсвующий колонке с ненулевым значением\n",
    "*   если в обеих колонках ненулевые значения и разность двух чисел меньше 3 — Neutral\n",
    "*   если в обеих колонках ненулевые значения и разность чисел не меньше 3 — лейбл, соответствующий колонке с наибольшим значением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем столбец из нулей\n",
    "\n",
    "dataframe.insert(13, \"arg_label\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем классы для аргументации\n",
    "\n",
    "# dataframe[\"arg_label\"] = np.where((dataframe[\"for\"] == 0 and dataframe[\"against\"] == 0), \"Neutral\", 0)\n",
    "\n",
    "arg_label = []\n",
    "for index, row in dataframe.iterrows():\n",
    "  if row[\"for\"] == 0 and row[\"against\"] == 0:\n",
    "    arg_label.append(\"Neutral\")\n",
    "  elif row[\"for\"] == 0:\n",
    "    arg_label.append(\"Bad\")\n",
    "  elif row[\"against\"] == 0:\n",
    "    arg_label.append(\"Good\")\n",
    "  elif abs(row[\"for\"] - row[\"against\"]) > 2:\n",
    "    if row[\"for\"] > row[\"against\"]:\n",
    "      arg_label.append(\"Good\")\n",
    "    elif row[\"for\"] < row[\"against\"]:\n",
    "      arg_label.append(\"Bad\")\n",
    "  elif abs(row[\"for\"] - row[\"against\"]) <= 2:\n",
    "    arg_label.append(\"Neutral\")\n",
    "  else:\n",
    "    arg_label.append(0)\n",
    "    print(row)\n",
    "\n",
    "dataframe[\"arg_label\"] = arg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем столбец \"век\"\n",
    "dataframe.insert(14, \"years\", 0)\n",
    "\n",
    "years = []\n",
    "for index, row in dataframe.iterrows():\n",
    "  # years.append(int(row[\"movie_name\"].split(\" \")[-1][1:-1]))\n",
    "  year = int(row[\"movie_name\"].split(\" \")[-1][1:-1])\n",
    "  if year <= 2000:\n",
    "    years.append(20)\n",
    "  else:\n",
    "    years.append(21)\n",
    "dataframe[\"years\"] = years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dataframe[\"years\"].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем столбец с рейтингом\n",
    "dataframe.insert(15, \"half\", 0)\n",
    "part = []\n",
    "for index, row in dataframe.iterrows():\n",
    "  part.append(int(row[\"part\"][-3:]))\n",
    "\n",
    "dataframe[\"half\"] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dataframe[\"half\"].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем столбец с соответствием\n",
    "dataframe.insert(16, \"congruence\", 0)\n",
    "are_congruent = []\n",
    "for index, row in dataframe.iterrows():\n",
    "  are_congruent.append(int(row[\"NEW_grade3\"] == row[\"arg_label\"]))\n",
    "\n",
    "dataframe[\"congruence\"] = are_congruent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dataframe[\"congruence\"].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш датасет можно увидеть по этой ссылке: https://huggingface.co/datasets/otipl2125/film_review_argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка обучающей, валидационной и тестовой выборок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем датасет на 3 выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataframe[0:2295]\n",
    "val = dataframe[2296:2788]\n",
    "test = dataframe[2788:]\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим метки из текстового формата в числовой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"Bad\": 0, \"Neutral\": 1, \"Good\": 2}\n",
    "train[\"raw_NEW_grade3\"] = train['NEW_grade3']\n",
    "train['NEW_grade3'] = train['raw_NEW_grade3'].map(label_dict)\n",
    "train['raw_arg_label'] = train['arg_label']\n",
    "train['arg_label'] = train[\"raw_arg_label\"].map(label_dict)\n",
    "val[\"raw_NEW_grade3\"] = val['NEW_grade3']\n",
    "val['NEW_grade3'] = val['raw_NEW_grade3'].map(label_dict)\n",
    "val['raw_arg_label'] = val['arg_label']\n",
    "val['arg_label'] = val[\"raw_arg_label\"].map(label_dict)\n",
    "test[\"raw_NEW_grade3\"] = test['NEW_grade3']\n",
    "test['NEW_grade3'] = test['raw_NEW_grade3'].map(label_dict)\n",
    "test['raw_arg_label'] = test['arg_label']\n",
    "test['arg_label'] = test[\"raw_arg_label\"].map(label_dict)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train, preserve_index=False)\n",
    "print(type(train))\n",
    "val = Dataset.from_pandas(val, preserve_index=False)\n",
    "test = Dataset.from_pandas(test, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train,\n",
    "                            \"validation\": val,\n",
    "                            \"test\": test})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы планируем использовать метод дообучения энкодерной модели и работать с русскоязычными текстами, воспользуемся моделью DeepPavlov/rubert-base-cased, с помощью которой было создано базовое решение организаторов RuArg-2022, а затем моделью ai-forever/ru-Roberta-large, учитывая опыт исследований, связанных с IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "#DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepPavlov/rubert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['Unnamed: 0', 'part', 'movie_name', \"author\", \"review_id\", \"date\", \"title\", \"grade10\", \"grade3\", \"content\", \"for\", \"against\", \"raw_NEW_grade3\", \"raw_arg_label\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объекты класса DataLoader для деления на батчи и паддинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дообучения модели для задачи классификации возьмем предобученную модель с незамороженными весами и добавим два линейных слоя: для определения позиции и для классификации доводов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CustomBertForSequenceClassification(nn.Module):\n",
    "\n",
    "  def __init__(self, n_labels):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(checkpoint)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.stance_out = nn.Linear(self.bert.config.hidden_size, n_labels)\n",
    "    self.argument_out = nn.Linear(self.bert.config.hidden_size, n_labels)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      return_dict=False)\n",
    "    output = self.drop(pooled_output)\n",
    "    stance_logits = self.stance_out(output)\n",
    "    argument_logits = self.argument_out(output)\n",
    "\n",
    "    return {\"stance\": stance_logits, \"argument\": argument_logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функции для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_epoch(current_class, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train() # переводим модель в состояние обучения\n",
    "\n",
    "  losses = [] # значения функции потерь\n",
    "  # значения accuracy\n",
    "  stance_correct_predictions = 0\n",
    "  argument_correct_predictions = 0\n",
    "\n",
    "  for d in data_loader: # итерация по батчам\n",
    "    input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
    "    attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
    "    # метки классов\n",
    "    stance_targets = d[f\"NEW_grade3\"].to(device)\n",
    "    argument_targets = d[f\"arg_label\"].to(device)\n",
    "\n",
    "    # применяем модель\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # позиция максимального значения\n",
    "    stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
    "    argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
    "    # подсчет функции потерь\n",
    "    stance_loss = loss_fn(outputs[\"stance\"], stance_targets)\n",
    "    argument_loss = loss_fn(outputs[\"argument\"], argument_targets)\n",
    "    loss = stance_loss + argument_loss\n",
    "\n",
    "    # количество совпадений\n",
    "    stance_correct_predictions += torch.sum(stance_preds == stance_targets)\n",
    "    argument_correct_predictions += torch.sum(argument_preds == argument_targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward() # подсчет градиента\n",
    "    optimizer.step() # обновление весов\n",
    "    scheduler.step() # изменение скорости обучения\n",
    "    optimizer.zero_grad() # обнуление градиентов\n",
    "\n",
    "  return stance_correct_predictions / n_examples, argument_correct_predictions / n_examples, np.mean(losses) # accuracy, среднее значение ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(current_class, model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval() # переводим модель в состояние валидации\n",
    "\n",
    "  losses = [] # значения функции потерь\n",
    "  # значения accuracy\n",
    "  stance_correct_predictions = 0\n",
    "  argument_correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad(): # градиент не считается\n",
    "    for d in data_loader: # итерация по батчам\n",
    "      input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
    "      attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
    "      # метки классов\n",
    "      stance_targets = d[f\"NEW_grade3\"].to(device)\n",
    "      argument_targets = d[f\"arg_label\"].to(device)\n",
    "\n",
    "      # применяем модель\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      # позиция максимального значения\n",
    "      stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
    "      argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
    "      # подсчет функции потерь\n",
    "      stance_loss = loss_fn(outputs[\"stance\"], stance_targets)\n",
    "      argument_loss = loss_fn(outputs[\"argument\"], argument_targets)\n",
    "      loss = stance_loss + argument_loss\n",
    "\n",
    "      # количество совпадений\n",
    "      stance_correct_predictions += torch.sum(stance_preds == stance_targets)\n",
    "      argument_correct_predictions += torch.sum(argument_preds == argument_targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return stance_correct_predictions / n_examples, argument_correct_predictions / n_examples, np.mean(losses) # accuracy, среднее значение ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def fine_tuning(current_class, epochs):\n",
    "\n",
    "  print(f\"Training model:\\n\")\n",
    "\n",
    "  # Загрузка предобученной модели\n",
    "  bert_model = AutoModel.from_pretrained(checkpoint)\n",
    "  # Добавление линейных слоев\n",
    "  model = CustomBertForSequenceClassification(n_labels = 4).to(device)\n",
    "\n",
    "  EPOCHS = epochs\n",
    "  # Обучение всех слоев\n",
    "  optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "  total_steps = len(train_dataloader) * EPOCHS\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "  loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "  for epoch in range(EPOCHS): # итерация по эпохам\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # обучение\n",
    "    train_stance_acc, train_argument_acc, train_loss = train_epoch(current_class, model, train_dataloader, loss_fn, optimizer, device, scheduler, len(tokenized_dataset[\"train\"]))\n",
    "\n",
    "    print(f'Train loss {train_loss} stance accuracy {train_stance_acc} argument accuracy {train_argument_acc}')\n",
    "\n",
    "    # валидация\n",
    "    val_stance_acc, val_argument_acc, val_loss = eval_model(current_class, model, val_dataloader, loss_fn, device, len(tokenized_dataset[\"test\"]))\n",
    "\n",
    "    print(f'Val loss {val_loss} stance accuracy {val_stance_acc} argument accuracy {val_argument_acc}')\n",
    "    print()\n",
    "\n",
    "  return bert_model, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model, supervised_model = fine_tuning(\"\", epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функции для получения предсказаний обученной модели и подсчета макро F1-меры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "\n",
    "  # предсказанные метки\n",
    "  stance_predictions = []\n",
    "  argument_predictions = []\n",
    "\n",
    "  with torch.no_grad(): # градиент не считается\n",
    "    for d in data_loader: # итерация по батчам\n",
    "      input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
    "      attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
    "\n",
    "      # применяем модель\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      # позиция максимального значения\n",
    "      stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
    "      argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
    "\n",
    "      stance_predictions.extend(stance_preds)\n",
    "      argument_predictions.extend(argument_preds)\n",
    "\n",
    "  stance_predictions = torch.stack(stance_predictions).cpu()\n",
    "  argument_predictions = torch.stack(argument_predictions).cpu()\n",
    "\n",
    "\n",
    "  stance_predictions = [x.item() for x in stance_predictions]\n",
    "  argument_predictions = [x.item() for x in argument_predictions]\n",
    "\n",
    "  return stance_predictions, argument_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    return metric.compute(predictions=preds, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score(model):\n",
    "  val_stance_predictions, val_argument_predictions = get_predictions(model, val_dataloader)\n",
    "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"stance_predicdions\", val_stance_predictions)\n",
    "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"argument_predicdions\", val_argument_predictions)\n",
    "  # filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[f\"NEW_grade3\"]!=-1)\n",
    "  stance_f1 = compute_metrics(tokenized_dataset[\"validation\"][f\"stance_predicdions\"], tokenized_dataset[\"validation\"][f\"NEW_grade3\"])\n",
    "  argument_f1 = compute_metrics(tokenized_dataset[\"validation\"][f\"argument_predicdions\"], tokenized_dataset[\"validation\"][f\"arg_label\"])\n",
    "  return stance_f1['f1'], argument_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_f1, argument_f1 = validation_score(supervised_model)\n",
    "print(f\"Stance F1 = {stance_f1}\")\n",
    "print(f\"Argument F1 = {argument_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели для фильмов XX и XXI века."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_years(model):\n",
    "\n",
    "\n",
    "    val_dataset_year_20 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"years\"] == 20)\n",
    "    val_dataset_year_21 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"years\"] == 21)\n",
    "\n",
    "    val_dataloader_for_year_20 = DataLoader(\n",
    "    val_dataset_year_20, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_year_21 = DataLoader(\n",
    "    val_dataset_year_21, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_20, val_argument_predictions_20 = get_predictions(model, val_dataloader_for_year_20)\n",
    "    val_dataset_year_20 = val_dataset_year_20.add_column(\"stance_predictions\", val_stance_predictions_20)\n",
    "    val_dataset_year_20 = val_dataset_year_20.add_column(\"argument_predictions\", val_argument_predictions_20)\n",
    "\n",
    "\n",
    "    stance_f1_20 = compute_metrics(val_dataset_year_20[\"stance_predictions\"], val_dataset_year_20[\"NEW_grade3\"])\n",
    "    argument_f1_20 = compute_metrics(val_dataset_year_20[\"argument_predictions\"], val_dataset_year_20[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_21, val_argument_predictions_21 = get_predictions(model, val_dataloader_for_year_21)\n",
    "    val_dataset_year_21 = val_dataset_year_21.add_column(\"stance_predictions\", val_stance_predictions_21)\n",
    "    val_dataset_year_21 = val_dataset_year_21.add_column(\"argument_predictions\", val_argument_predictions_21)\n",
    "\n",
    "\n",
    "    stance_f1_21 = compute_metrics(val_dataset_year_21[\"stance_predictions\"], val_dataset_year_21[\"NEW_grade3\"])\n",
    "    argument_f1_21 = compute_metrics(val_dataset_year_21[\"argument_predictions\"], val_dataset_year_21[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_20['f1'], argument_f1_20['f1']), (stance_f1_21['f1'], argument_f1_21['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_20, f1_21 = validation_score_years(supervised_model)\n",
    "print(f\"Century 20 = {f1_20}\")\n",
    "print(f\"Century 21 = {f1_21}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели для фильмов из вершины рейтинга (top-250) и низа рейтинга (bottom-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_parts(model):\n",
    "\n",
    "\n",
    "    val_dataset_part_250 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"half\"] == 250)\n",
    "    val_dataset_part_100 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"half\"] == 100)\n",
    "\n",
    "    val_dataloader_for_part_250 = DataLoader(\n",
    "    val_dataset_part_250, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_part_100 = DataLoader(\n",
    "    val_dataset_part_100, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_part_250, val_argument_predictions_part_250 = get_predictions(model, val_dataloader_for_part_250)\n",
    "    val_dataset_part_250 = val_dataset_part_250.add_column(\"stance_predictions\", val_stance_predictions_part_250)\n",
    "    val_dataset_part_250 = val_dataset_part_250.add_column(\"argument_predictions\", val_argument_predictions_part_250)\n",
    "\n",
    "\n",
    "    stance_f1_part_250 = compute_metrics(val_dataset_part_250[\"stance_predictions\"], val_dataset_part_250[\"NEW_grade3\"])\n",
    "    argument_f1_part_250 = compute_metrics(val_dataset_part_250[\"argument_predictions\"], val_dataset_part_250[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_part_100, val_argument_predictions_part_100 = get_predictions(model, val_dataloader_for_part_100)\n",
    "    val_dataset_part_100 = val_dataset_part_100.add_column(\"stance_predictions\", val_stance_predictions_part_100)\n",
    "    val_dataset_part_100 = val_dataset_part_100.add_column(\"argument_predictions\", val_argument_predictions_part_100)\n",
    "\n",
    "\n",
    "    stance_f1_part_100 = compute_metrics(val_dataset_part_100[\"stance_predictions\"], val_dataset_part_100[\"NEW_grade3\"])\n",
    "    argument_f1_part_100 = compute_metrics(val_dataset_part_100[\"argument_predictions\"], val_dataset_part_100[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_part_250['f1'], argument_f1_part_250['f1']), (stance_f1_part_100['f1'], argument_f1_part_100['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_250_f1, part_100_f1 = validation_score_parts(supervised_model)\n",
    "print(f\"Part 250 = {part_250_f1}\")\n",
    "print(f\"Part 100 = {part_100_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели для конгруэнтных и неконгруэнтных меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_congruence(model):\n",
    "\n",
    "\n",
    "    val_dataset_congruent = tokenized_dataset[\"validation\"].filter(lambda example: example[\"congruence\"] == 1)\n",
    "    val_dataset_not_congruent = tokenized_dataset[\"validation\"].filter(lambda example: example[\"congruence\"] == 0)\n",
    "\n",
    "    val_dataloader_for_congruent = DataLoader(\n",
    "    val_dataset_congruent, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_not_congruent = DataLoader(\n",
    "    val_dataset_not_congruent, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_congruent, val_argument_predictions_congruent = get_predictions(model, val_dataloader_for_congruent)\n",
    "    val_dataset_congruent = val_dataset_congruent.add_column(\"stance_predictions\", val_stance_predictions_congruent)\n",
    "    val_dataset_congruent = val_dataset_congruent.add_column(\"argument_predictions\", val_argument_predictions_congruent)\n",
    "\n",
    "\n",
    "    stance_f1_congruent = compute_metrics(val_dataset_congruent[\"stance_predictions\"], val_dataset_congruent[\"NEW_grade3\"])\n",
    "    argument_f1_congruent = compute_metrics(val_dataset_congruent[\"argument_predictions\"], val_dataset_congruent[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_not_congruent, val_argument_predictions_not_congruent = get_predictions(model, val_dataloader_for_not_congruent)\n",
    "    val_dataset_not_congruent = val_dataset_not_congruent.add_column(\"stance_predictions\", val_stance_predictions_not_congruent)\n",
    "    val_dataset_not_congruent = val_dataset_not_congruent.add_column(\"argument_predictions\", val_argument_predictions_not_congruent)\n",
    "\n",
    "\n",
    "    stance_f1_not_congruent = compute_metrics(val_dataset_not_congruent[\"stance_predictions\"], val_dataset_not_congruent[\"NEW_grade3\"])\n",
    "    argument_f1_not_congruent = compute_metrics(val_dataset_not_congruent[\"argument_predictions\"], val_dataset_not_congruent[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_congruent['f1'], argument_f1_congruent['f1']), (stance_f1_not_congruent['f1'], argument_f1_not_congruent['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congruent_f1, not_congruent_f1 = validation_score_congruence(supervised_model)\n",
    "print(f\"Congruent = {congruent_f1}\")\n",
    "print(f\"Not congruent = {not_congruent_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ai-forever/ru-Roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"ai-forever/ruRoberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruRoberta-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['Unnamed: 0', 'part', 'movie_name', \"author\", \"review_id\", \"date\", \"title\", \"grade10\", \"grade3\", \"content\", \"for\", \"against\", \"raw_NEW_grade3\", \"raw_arg_label\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объекты класса DataLoader для деления на батчи и паддинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model, supervised_model = fine_tuning(\"\", epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score(model):\n",
    "  val_stance_predictions, val_argument_predictions = get_predictions(model, val_dataloader)\n",
    "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"stance_predictions\", val_stance_predictions)\n",
    "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"argument_predictions\", val_argument_predictions)\n",
    "  # filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[f\"NEW_grade3\"]!=-1)\n",
    "  stance_f1 = compute_metrics(tokenized_dataset[\"validation\"][f\"stance_predictions\"], tokenized_dataset[\"validation\"][f\"NEW_grade3\"])\n",
    "  argument_f1 = compute_metrics(tokenized_dataset[\"validation\"][f\"argument_predictions\"], tokenized_dataset[\"validation\"][f\"arg_label\"])\n",
    "  return stance_f1['f1'], argument_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_f1, argument_f1 = validation_score(supervised_model)\n",
    "print(f\"Stance F1 = {stance_f1}\")\n",
    "print(f\"Argument F1 = {argument_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели для фильмов XX и XXI вв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_years(model):\n",
    "\n",
    "\n",
    "    val_dataset_year_20 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"years\"] == 20)\n",
    "    val_dataset_year_21 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"years\"] == 21)\n",
    "\n",
    "    val_dataloader_for_year_20 = DataLoader(\n",
    "    val_dataset_year_20, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_year_21 = DataLoader(\n",
    "    val_dataset_year_21, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_20, val_argument_predictions_20 = get_predictions(model, val_dataloader_for_year_20)\n",
    "    val_dataset_year_20 = val_dataset_year_20.add_column(\"stance_predictions\", val_stance_predictions_20)\n",
    "    val_dataset_year_20 = val_dataset_year_20.add_column(\"argument_predictions\", val_argument_predictions_20)\n",
    "\n",
    "\n",
    "    stance_f1_20 = compute_metrics(val_dataset_year_20[\"stance_predictions\"], val_dataset_year_20[\"NEW_grade3\"])\n",
    "    argument_f1_20 = compute_metrics(val_dataset_year_20[\"argument_predictions\"], val_dataset_year_20[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_21, val_argument_predictions_21 = get_predictions(model, val_dataloader_for_year_21)\n",
    "    val_dataset_year_21 = val_dataset_year_21.add_column(\"stance_predictions\", val_stance_predictions_21)\n",
    "    val_dataset_year_21 = val_dataset_year_21.add_column(\"argument_predictions\", val_argument_predictions_21)\n",
    "\n",
    "\n",
    "    stance_f1_21 = compute_metrics(val_dataset_year_21[\"stance_predictions\"], val_dataset_year_21[\"NEW_grade3\"])\n",
    "    argument_f1_21 = compute_metrics(val_dataset_year_21[\"argument_predictions\"], val_dataset_year_21[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_20['f1'], argument_f1_20['f1']), (stance_f1_21['f1'], argument_f1_21['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_f1, argument_f1 = validation_score_years(supervised_model)\n",
    "print(f\"Century 20 = {stance_f1}\")\n",
    "print(f\"Century 21 = {argument_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрику для модели для фильмов из вершины рейтинга (top-250) и низа рейтинга (bottom-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_parts(model):\n",
    "\n",
    "\n",
    "    val_dataset_part_250 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"half\"] == 250)\n",
    "    val_dataset_part_100 = tokenized_dataset[\"validation\"].filter(lambda example: example[\"half\"] == 100)\n",
    "\n",
    "    val_dataloader_for_part_250 = DataLoader(\n",
    "    val_dataset_part_250, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_part_100 = DataLoader(\n",
    "    val_dataset_part_100, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_part_250, val_argument_predictions_part_250 = get_predictions(model, val_dataloader_for_part_250)\n",
    "    val_dataset_part_250 = val_dataset_part_250.add_column(\"stance_predictions\", val_stance_predictions_part_250)\n",
    "    val_dataset_part_250 = val_dataset_part_250.add_column(\"argument_predictions\", val_argument_predictions_part_250)\n",
    "\n",
    "\n",
    "    stance_f1_part_250 = compute_metrics(val_dataset_part_250[\"stance_predictions\"], val_dataset_part_250[\"NEW_grade3\"])\n",
    "    argument_f1_part_250 = compute_metrics(val_dataset_part_250[\"argument_predictions\"], val_dataset_part_250[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_part_100, val_argument_predictions_part_100 = get_predictions(model, val_dataloader_for_part_100)\n",
    "    val_dataset_part_100 = val_dataset_part_100.add_column(\"stance_predictions\", val_stance_predictions_part_100)\n",
    "    val_dataset_part_100 = val_dataset_part_100.add_column(\"argument_predictions\", val_argument_predictions_part_100)\n",
    "\n",
    "\n",
    "    stance_f1_part_100 = compute_metrics(val_dataset_part_100[\"stance_predictions\"], val_dataset_part_100[\"NEW_grade3\"])\n",
    "    argument_f1_part_100 = compute_metrics(val_dataset_part_100[\"argument_predictions\"], val_dataset_part_100[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_part_250['f1'], argument_f1_part_250['f1']), (stance_f1_part_100['f1'], argument_f1_part_100['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_f1, argument_f1 = validation_score_parts(supervised_model)\n",
    "print(f\"Part 250 = {stance_f1}\")\n",
    "print(f\"Part 100 = {argument_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем метрику для конгруэнтных и неконгруэнтных меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_score_congruence(model):\n",
    "\n",
    "\n",
    "    val_dataset_congruent = tokenized_dataset[\"validation\"].filter(lambda example: example[\"congruence\"] == 1)\n",
    "    val_dataset_not_congruent = tokenized_dataset[\"validation\"].filter(lambda example: example[\"congruence\"] == 0)\n",
    "\n",
    "    val_dataloader_for_congruent = DataLoader(\n",
    "    val_dataset_congruent, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_dataloader_for_not_congruent = DataLoader(\n",
    "    val_dataset_not_congruent, shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    val_stance_predictions_congruent, val_argument_predictions_congruent = get_predictions(model, val_dataloader_for_congruent)\n",
    "    val_dataset_congruent = val_dataset_congruent.add_column(\"stance_predictions\", val_stance_predictions_congruent)\n",
    "    val_dataset_congruent = val_dataset_congruent.add_column(\"argument_predictions\", val_argument_predictions_congruent)\n",
    "\n",
    "\n",
    "    stance_f1_congruent = compute_metrics(val_dataset_congruent[\"stance_predictions\"], val_dataset_congruent[\"NEW_grade3\"])\n",
    "    argument_f1_congruent = compute_metrics(val_dataset_congruent[\"argument_predictions\"], val_dataset_congruent[\"arg_label\"])\n",
    "\n",
    "\n",
    "    val_stance_predictions_not_congruent, val_argument_predictions_not_congruent = get_predictions(model, val_dataloader_for_not_congruent)\n",
    "    val_dataset_not_congruent = val_dataset_not_congruent.add_column(\"stance_predictions\", val_stance_predictions_not_congruent)\n",
    "    val_dataset_not_congruent = val_dataset_not_congruent.add_column(\"argument_predictions\", val_argument_predictions_not_congruent)\n",
    "\n",
    "\n",
    "    stance_f1_not_congruent = compute_metrics(val_dataset_not_congruent[\"stance_predictions\"], val_dataset_not_congruent[\"NEW_grade3\"])\n",
    "    argument_f1_not_congruent = compute_metrics(val_dataset_not_congruent[\"argument_predictions\"], val_dataset_not_congruent[\"arg_label\"])\n",
    "\n",
    "    return (stance_f1_congruent['f1'], argument_f1_congruent['f1']), (stance_f1_not_congruent['f1'], argument_f1_not_congruent['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_f1, argument_f1 = validation_score_congruence(supervised_model)\n",
    "print(f\"Congruent = {stance_f1}\")\n",
    "print(f\"Not congruent = {argument_f1}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
